{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtjmBlt_uBDB",
    "outputId": "9d9f413d-18a2-49a4-8089-4dd340188065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#\n",
    "#import csv\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',500)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import mca\n",
    "#\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "#\n",
    "from tensorflow.keras.layers import Dense, Input, Activation\n",
    "#\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, cluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#\n",
    "#\n",
    "from keras.models import Sequential  # Model type to be used\n",
    "\n",
    "#from tensorflow.keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dense, Input, Activation, Dropout, LeakyReLU\n",
    "leaky_relu = LeakyReLU(alpha=0.01)\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#\n",
    "from tensorflow.keras import callbacks\n",
    "#\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(tf.__version__)\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hShZj8sD-6Ee"
   },
   "source": [
    "#Data importing, cleaning and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AfTPNRh9u-NC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0017A6SJgTbfQVU2EtsPNo</td>\n",
       "      <td>Pangarap</td>\n",
       "      <td>Barbie's Cradle</td>\n",
       "      <td>Minsan pa Nang ako'y napalingon Hindi ko alam ...</td>\n",
       "      <td>41</td>\n",
       "      <td>1srJQ0njEQgd8w4XSqI4JQ</td>\n",
       "      <td>Trip</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.27900</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.566</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004s3t0ONYlzxII9PLgU6z</td>\n",
       "      <td>I Feel Alive</td>\n",
       "      <td>Steady Rollin</td>\n",
       "      <td>The trees, are singing in the wind The sky blu...</td>\n",
       "      <td>28</td>\n",
       "      <td>3z04Lb9Dsilqw68SHt6jLB</td>\n",
       "      <td>Love &amp; Loss</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>Hard Rock Workout</td>\n",
       "      <td>3YouF0u7waJnolytf9JCXf</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00chLpzhgVjxs1zKC9UScL</td>\n",
       "      <td>Poison</td>\n",
       "      <td>Bell Biv DeVoe</td>\n",
       "      <td>NA Yeah, Spyderman and Freeze in full effect U...</td>\n",
       "      <td>0</td>\n",
       "      <td>6oZ6brjB8x3GoeSYdwJdPc</td>\n",
       "      <td>Gold</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>Back in the day - R&amp;B, New Jack Swing, Swingbe...</td>\n",
       "      <td>3a9y4eeCJRmG9p4YKfqYIx</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00cqd6ZsSkLZqGMlQCR0Zo</td>\n",
       "      <td>Baby It's Cold Outside (feat. Christina Aguilera)</td>\n",
       "      <td>CeeLo Green</td>\n",
       "      <td>I really can't stay Baby it's cold outside I'v...</td>\n",
       "      <td>41</td>\n",
       "      <td>3ssspRe42CXkhPxdc12xcp</td>\n",
       "      <td>CeeLo's Magic Moment</td>\n",
       "      <td>2012-10-29</td>\n",
       "      <td>Christmas Soul</td>\n",
       "      <td>6FZYc2BvF7tColxO8PBShV</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00emjlCv9azBN0fzuuyLqy</td>\n",
       "      <td>Dumb Litty</td>\n",
       "      <td>KARD</td>\n",
       "      <td>Get up out of my business You don't keep me fr...</td>\n",
       "      <td>65</td>\n",
       "      <td>7h5X3xhh3peIK9Y0qI5hbK</td>\n",
       "      <td>KARD 2nd Digital Single ‘Dumb Litty’</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>K-Party Dance Mix</td>\n",
       "      <td>37i9dQZF1DX4RDXswvP6Mj</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.240</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18449</th>\n",
       "      <td>7zXzWCVuz7whIuYZyEAlxt</td>\n",
       "      <td>Rising Like The Sun - Radio Mix</td>\n",
       "      <td>Qulinez</td>\n",
       "      <td>Caught up in such a head rush, wide-eyed latel...</td>\n",
       "      <td>0</td>\n",
       "      <td>1l4aoukbPgi5u2OaE2R4Zj</td>\n",
       "      <td>Rising Like The Sun</td>\n",
       "      <td>2014-03-24</td>\n",
       "      <td>♥ EDM LOVE 2020</td>\n",
       "      <td>6jI1gFr6ANFtT8MmTvA2Ux</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.211</td>\n",
       "      <td>128.012</td>\n",
       "      <td>208656</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>7zycSpvjDcqh6YT1FEl2kY</td>\n",
       "      <td>Anaconda</td>\n",
       "      <td>Nicki Minaj</td>\n",
       "      <td>My anaconda don't, my anaconda don't My anacon...</td>\n",
       "      <td>49</td>\n",
       "      <td>5qs8T6ZHSrnllnOuUk6muC</td>\n",
       "      <td>The Pinkprint (Deluxe Edition)</td>\n",
       "      <td>2014-12-15</td>\n",
       "      <td>10er Playlist</td>\n",
       "      <td>1kEczIkZH8IgaWT2BiApxZ</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.06730</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.647</td>\n",
       "      <td>129.990</td>\n",
       "      <td>260240</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>7zye9v6B785eFWEFYs13C2</td>\n",
       "      <td>Bound</td>\n",
       "      <td>Ponderosa Twins Plus One</td>\n",
       "      <td>Bound, bound Bound, bound Bound to fall in lov...</td>\n",
       "      <td>40</td>\n",
       "      <td>1xdgLmTFMSyJyI5DJOOX7T</td>\n",
       "      <td>2+2+1 = (Digitally Remastered)</td>\n",
       "      <td>2013-07-09</td>\n",
       "      <td>Sexy Soul 2020</td>\n",
       "      <td>5EMARioe9z9eKOeWIAC2JW</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.71500</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.657</td>\n",
       "      <td>142.218</td>\n",
       "      <td>191205</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18452</th>\n",
       "      <td>7zyLObYw4QUKQDyZOb4J0Y</td>\n",
       "      <td>I'll Do 4 U (Re-Recorded / Remastered)</td>\n",
       "      <td>Father MC</td>\n",
       "      <td>(Would you do for me) Sweetheart (Would you do...</td>\n",
       "      <td>36</td>\n",
       "      <td>14HYMxFhpgDIr9cci1u0kt</td>\n",
       "      <td>I'll Do 4 U (Re-Recorded / Remastered)</td>\n",
       "      <td>2010-10-01</td>\n",
       "      <td>New Jack Swing/ R&amp;B Hits: 1987 - 2002</td>\n",
       "      <td>4sji14lrB5bgcr51lPALYH</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.920</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.14300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.810</td>\n",
       "      <td>109.536</td>\n",
       "      <td>223890</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18453</th>\n",
       "      <td>7zzZmpw8L66ZPjH1M6qmOs</td>\n",
       "      <td>Migraine</td>\n",
       "      <td>Moonstar88</td>\n",
       "      <td>Oo nga pala, hindi nga pala tayo Hanggang dito...</td>\n",
       "      <td>61</td>\n",
       "      <td>4t3FtECyV1gClHmpBhXSfB</td>\n",
       "      <td>When I Met You</td>\n",
       "      <td>2008-07-28</td>\n",
       "      <td>Pinoy Classic Rock</td>\n",
       "      <td>37i9dQZF1DWYDQ8wBxd7xt</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.21700</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.221</td>\n",
       "      <td>115.049</td>\n",
       "      <td>267960</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18454 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     track_id  \\\n",
       "0      0017A6SJgTbfQVU2EtsPNo   \n",
       "1      004s3t0ONYlzxII9PLgU6z   \n",
       "2      00chLpzhgVjxs1zKC9UScL   \n",
       "3      00cqd6ZsSkLZqGMlQCR0Zo   \n",
       "4      00emjlCv9azBN0fzuuyLqy   \n",
       "...                       ...   \n",
       "18449  7zXzWCVuz7whIuYZyEAlxt   \n",
       "18450  7zycSpvjDcqh6YT1FEl2kY   \n",
       "18451  7zye9v6B785eFWEFYs13C2   \n",
       "18452  7zyLObYw4QUKQDyZOb4J0Y   \n",
       "18453  7zzZmpw8L66ZPjH1M6qmOs   \n",
       "\n",
       "                                              track_name  \\\n",
       "0                                               Pangarap   \n",
       "1                                           I Feel Alive   \n",
       "2                                                 Poison   \n",
       "3      Baby It's Cold Outside (feat. Christina Aguilera)   \n",
       "4                                             Dumb Litty   \n",
       "...                                                  ...   \n",
       "18449                    Rising Like The Sun - Radio Mix   \n",
       "18450                                           Anaconda   \n",
       "18451                                              Bound   \n",
       "18452             I'll Do 4 U (Re-Recorded / Remastered)   \n",
       "18453                                           Migraine   \n",
       "\n",
       "                   track_artist  \\\n",
       "0               Barbie's Cradle   \n",
       "1                 Steady Rollin   \n",
       "2                Bell Biv DeVoe   \n",
       "3                   CeeLo Green   \n",
       "4                          KARD   \n",
       "...                         ...   \n",
       "18449                   Qulinez   \n",
       "18450               Nicki Minaj   \n",
       "18451  Ponderosa Twins Plus One   \n",
       "18452                 Father MC   \n",
       "18453                Moonstar88   \n",
       "\n",
       "                                                  lyrics  track_popularity  \\\n",
       "0      Minsan pa Nang ako'y napalingon Hindi ko alam ...                41   \n",
       "1      The trees, are singing in the wind The sky blu...                28   \n",
       "2      NA Yeah, Spyderman and Freeze in full effect U...                 0   \n",
       "3      I really can't stay Baby it's cold outside I'v...                41   \n",
       "4      Get up out of my business You don't keep me fr...                65   \n",
       "...                                                  ...               ...   \n",
       "18449  Caught up in such a head rush, wide-eyed latel...                 0   \n",
       "18450  My anaconda don't, my anaconda don't My anacon...                49   \n",
       "18451  Bound, bound Bound, bound Bound to fall in lov...                40   \n",
       "18452  (Would you do for me) Sweetheart (Would you do...                36   \n",
       "18453  Oo nga pala, hindi nga pala tayo Hanggang dito...                61   \n",
       "\n",
       "               track_album_id                        track_album_name  \\\n",
       "0      1srJQ0njEQgd8w4XSqI4JQ                                    Trip   \n",
       "1      3z04Lb9Dsilqw68SHt6jLB                             Love & Loss   \n",
       "2      6oZ6brjB8x3GoeSYdwJdPc                                    Gold   \n",
       "3      3ssspRe42CXkhPxdc12xcp                    CeeLo's Magic Moment   \n",
       "4      7h5X3xhh3peIK9Y0qI5hbK    KARD 2nd Digital Single ‘Dumb Litty’   \n",
       "...                       ...                                     ...   \n",
       "18449  1l4aoukbPgi5u2OaE2R4Zj                     Rising Like The Sun   \n",
       "18450  5qs8T6ZHSrnllnOuUk6muC          The Pinkprint (Deluxe Edition)   \n",
       "18451  1xdgLmTFMSyJyI5DJOOX7T          2+2+1 = (Digitally Remastered)   \n",
       "18452  14HYMxFhpgDIr9cci1u0kt  I'll Do 4 U (Re-Recorded / Remastered)   \n",
       "18453  4t3FtECyV1gClHmpBhXSfB                          When I Met You   \n",
       "\n",
       "      track_album_release_date  \\\n",
       "0                   2001-01-01   \n",
       "1                   2017-11-21   \n",
       "2                   2005-01-01   \n",
       "3                   2012-10-29   \n",
       "4                   2019-09-22   \n",
       "...                        ...   \n",
       "18449               2014-03-24   \n",
       "18450               2014-12-15   \n",
       "18451               2013-07-09   \n",
       "18452               2010-10-01   \n",
       "18453               2008-07-28   \n",
       "\n",
       "                                           playlist_name  \\\n",
       "0                                     Pinoy Classic Rock   \n",
       "1                                      Hard Rock Workout   \n",
       "2      Back in the day - R&B, New Jack Swing, Swingbe...   \n",
       "3                                         Christmas Soul   \n",
       "4                                      K-Party Dance Mix   \n",
       "...                                                  ...   \n",
       "18449                                    ♥ EDM LOVE 2020   \n",
       "18450                                      10er Playlist   \n",
       "18451                                     Sexy Soul 2020   \n",
       "18452              New Jack Swing/ R&B Hits: 1987 - 2002   \n",
       "18453                                 Pinoy Classic Rock   \n",
       "\n",
       "                  playlist_id  ... loudness mode  speechiness  acousticness  \\\n",
       "0      37i9dQZF1DWYDQ8wBxd7xt  ...  -10.068    1       0.0236       0.27900   \n",
       "1      3YouF0u7waJnolytf9JCXf  ...   -4.739    1       0.0442       0.01170   \n",
       "2      3a9y4eeCJRmG9p4YKfqYIx  ...   -7.504    0       0.2160       0.00432   \n",
       "3      6FZYc2BvF7tColxO8PBShV  ...   -5.819    0       0.0341       0.68900   \n",
       "4      37i9dQZF1DX4RDXswvP6Mj  ...   -1.993    1       0.0409       0.03700   \n",
       "...                       ...  ...      ...  ...          ...           ...   \n",
       "18449  6jI1gFr6ANFtT8MmTvA2Ux  ...   -5.778    0       0.0878       0.00555   \n",
       "18450  1kEczIkZH8IgaWT2BiApxZ  ...   -6.224    1       0.1800       0.06730   \n",
       "18451  5EMARioe9z9eKOeWIAC2JW  ...   -6.457    0       0.0270       0.71500   \n",
       "18452  4sji14lrB5bgcr51lPALYH  ...   -4.920    0       0.0633       0.14300   \n",
       "18453  37i9dQZF1DWYDQ8wBxd7xt  ...   -6.000    1       0.0290       0.21700   \n",
       "\n",
       "       instrumentalness  liveness  valence    tempo  duration_ms  language  \n",
       "0              0.011700    0.0887    0.566   97.091       235440        tl  \n",
       "1              0.009940    0.3470    0.404  135.225       373512        en  \n",
       "2              0.007230    0.4890    0.650  111.904       262467        en  \n",
       "3              0.000000    0.0664    0.405  118.593       243067        en  \n",
       "4              0.000000    0.1380    0.240  130.018       193160        en  \n",
       "...                 ...       ...      ...      ...          ...       ...  \n",
       "18449          0.000000    0.3350    0.211  128.012       208656        en  \n",
       "18450          0.000006    0.2140    0.647  129.990       260240        en  \n",
       "18451          0.000428    0.1150    0.657  142.218       191205        en  \n",
       "18452          0.000000    0.0720    0.810  109.536       223890        en  \n",
       "18453          0.000003    0.1180    0.221  115.049       267960        tl  \n",
       "\n",
       "[18454 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/users/user/Documents/Untitled Folder/Datos/spotify_songs.csv') #embeding artist:name:lyrics\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EOJHd2UEvxMj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>language</th>\n",
       "      <th>days_since_release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>rock</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.401</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.27900</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.566</td>\n",
       "      <td>97.091</td>\n",
       "      <td>235440</td>\n",
       "      <td>tl</td>\n",
       "      <td>16071.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>rock</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.880</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>en</td>\n",
       "      <td>22239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.652</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>en</td>\n",
       "      <td>17532.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.378</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>en</td>\n",
       "      <td>20390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.887</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.240</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>en</td>\n",
       "      <td>22909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18449</th>\n",
       "      <td>0</td>\n",
       "      <td>edm</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.800</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.211</td>\n",
       "      <td>128.012</td>\n",
       "      <td>208656</td>\n",
       "      <td>en</td>\n",
       "      <td>20901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>49</td>\n",
       "      <td>pop</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.603</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.06730</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.647</td>\n",
       "      <td>129.990</td>\n",
       "      <td>260240</td>\n",
       "      <td>en</td>\n",
       "      <td>21167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>40</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.540</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.71500</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.657</td>\n",
       "      <td>142.218</td>\n",
       "      <td>191205</td>\n",
       "      <td>en</td>\n",
       "      <td>20643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18452</th>\n",
       "      <td>36</td>\n",
       "      <td>r&amp;b</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.666</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.920</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.14300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.810</td>\n",
       "      <td>109.536</td>\n",
       "      <td>223890</td>\n",
       "      <td>en</td>\n",
       "      <td>19631.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18453</th>\n",
       "      <td>61</td>\n",
       "      <td>rock</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.537</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.21700</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.221</td>\n",
       "      <td>115.049</td>\n",
       "      <td>267960</td>\n",
       "      <td>tl</td>\n",
       "      <td>18836.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18454 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track_popularity playlist_genre  danceability  energy  key  loudness  \\\n",
       "0                    41           rock         0.682   0.401    2   -10.068   \n",
       "1                    28           rock         0.303   0.880    9    -4.739   \n",
       "2                     0            r&b         0.845   0.652    6    -7.504   \n",
       "3                    41            r&b         0.425   0.378    5    -5.819   \n",
       "4                    65            pop         0.760   0.887    9    -1.993   \n",
       "...                 ...            ...           ...     ...  ...       ...   \n",
       "18449                 0            edm         0.479   0.800   10    -5.778   \n",
       "18450                49            pop         0.963   0.603    2    -6.224   \n",
       "18451                40            r&b         0.458   0.540    5    -6.457   \n",
       "18452                36            r&b         0.832   0.666    1    -4.920   \n",
       "18453                61           rock         0.664   0.537    2    -6.000   \n",
       "\n",
       "       mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0         1       0.0236       0.27900          0.011700    0.0887    0.566   \n",
       "1         1       0.0442       0.01170          0.009940    0.3470    0.404   \n",
       "2         0       0.2160       0.00432          0.007230    0.4890    0.650   \n",
       "3         0       0.0341       0.68900          0.000000    0.0664    0.405   \n",
       "4         1       0.0409       0.03700          0.000000    0.1380    0.240   \n",
       "...     ...          ...           ...               ...       ...      ...   \n",
       "18449     0       0.0878       0.00555          0.000000    0.3350    0.211   \n",
       "18450     1       0.1800       0.06730          0.000006    0.2140    0.647   \n",
       "18451     0       0.0270       0.71500          0.000428    0.1150    0.657   \n",
       "18452     0       0.0633       0.14300          0.000000    0.0720    0.810   \n",
       "18453     1       0.0290       0.21700          0.000003    0.1180    0.221   \n",
       "\n",
       "         tempo  duration_ms language  days_since_release  \n",
       "0       97.091       235440       tl             16071.0  \n",
       "1      135.225       373512       en             22239.0  \n",
       "2      111.904       262467       en             17532.0  \n",
       "3      118.593       243067       en             20390.0  \n",
       "4      130.018       193160       en             22909.0  \n",
       "...        ...          ...      ...                 ...  \n",
       "18449  128.012       208656       en             20901.0  \n",
       "18450  129.990       260240       en             21167.0  \n",
       "18451  142.218       191205       en             20643.0  \n",
       "18452  109.536       223890       en             19631.0  \n",
       "18453  115.049       267960       tl             18836.0  \n",
       "\n",
       "[18454 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's clean the data by dropping some columns\n",
    "dfnum   = df\n",
    "dfnum   = dfnum.drop(['playlist_subgenre','track_id','track_name', 'track_artist','lyrics','track_album_id',\t'track_album_name','playlist_name',\t'playlist_id'], axis=1)\n",
    "#Now, let's convert date of release to days since realise\n",
    "dfnum['track_album_release_date'] = pd.to_datetime(dfnum['track_album_release_date'])    \n",
    "dfnum['days_since_release'] = (dfnum['track_album_release_date'] - dfnum['track_album_release_date'].min())  / np.timedelta64(1,'D')\n",
    "dfnum   = dfnum.drop('track_album_release_date',axis=1)\n",
    "dfnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uilYNCuu7Vbr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_popularity        int64\n",
       "playlist_genre         object\n",
       "danceability          float64\n",
       "energy                float64\n",
       "key                     int64\n",
       "loudness              float64\n",
       "mode                    int64\n",
       "speechiness           float64\n",
       "acousticness          float64\n",
       "instrumentalness      float64\n",
       "liveness              float64\n",
       "valence               float64\n",
       "tempo                 float64\n",
       "duration_ms             int64\n",
       "language               object\n",
       "days_since_release    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnum.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMykqi380Tnp"
   },
   "source": [
    "Some columns provide qualitative information that would be useful for the predictor. Take, for example, the song's genre. I want to assing a different integer to each unique value of that column ( rock=0, pop=1, etc.) so then I will be able to use it as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OYxOEABn88zV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre_edm</th>\n",
       "      <th>genre_latin</th>\n",
       "      <th>genre_pop</th>\n",
       "      <th>genre_r&amp;b</th>\n",
       "      <th>genre_rap</th>\n",
       "      <th>genre_rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18449</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18452</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18454 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       genre_edm  genre_latin  genre_pop  genre_r&b  genre_rap  genre_rock\n",
       "0              0            0          0          0          0           1\n",
       "1              0            0          0          0          0           1\n",
       "2              0            0          0          1          0           0\n",
       "3              0            0          0          1          0           0\n",
       "4              0            0          1          0          0           0\n",
       "...          ...          ...        ...        ...        ...         ...\n",
       "18449          1            0          0          0          0           0\n",
       "18450          0            0          1          0          0           0\n",
       "18451          0            0          0          1          0           0\n",
       "18452          0            0          0          1          0           0\n",
       "18453          0            0          0          0          0           1\n",
       "\n",
       "[18454 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numerification of playlist_genre with one-hot encoding\n",
    "y = pd.get_dummies(dfnum.playlist_genre, prefix='genre')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eYBtWgGJ9GC0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23623753 0.18732462 0.16783144 0.15981959 0.10957926 0.06521964]\n",
      "0.9260120863101037\n"
     ]
    }
   ],
   "source": [
    "#numerification of language\n",
    "z = pd.get_dummies(dfnum.language, prefix='language')\n",
    "dfqual  =pd.concat([y,z],axis=1)\n",
    "pca = PCA(n_components=6)\n",
    "dfqual = pca.fit_transform(dfqual)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "dfqual = pd.DataFrame(data = dfqual, \n",
    "                  columns = ['qual0', 'qual1', 'qual2', 'qual3', 'qual4', 'qual5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L_NXJn9N9cl3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>days_since_release</th>\n",
       "      <th>qual0</th>\n",
       "      <th>qual1</th>\n",
       "      <th>qual2</th>\n",
       "      <th>qual3</th>\n",
       "      <th>qual4</th>\n",
       "      <th>qual5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.662692</td>\n",
       "      <td>0.390827</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.281249</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>0.080414</td>\n",
       "      <td>0.571136</td>\n",
       "      <td>0.338981</td>\n",
       "      <td>0.418893</td>\n",
       "      <td>0.697950</td>\n",
       "      <td>3.878689e-01</td>\n",
       "      <td>0.053253</td>\n",
       "      <td>9.561615e-01</td>\n",
       "      <td>0.300293</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.057691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.232254</td>\n",
       "      <td>0.877962</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.830868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>0.011793</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>0.342212</td>\n",
       "      <td>0.407663</td>\n",
       "      <td>0.554509</td>\n",
       "      <td>0.703040</td>\n",
       "      <td>0.965821</td>\n",
       "      <td>3.277147e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.137683e-01</td>\n",
       "      <td>0.383860</td>\n",
       "      <td>0.143633</td>\n",
       "      <td>0.440446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.847814</td>\n",
       "      <td>0.646090</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.753108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216168</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.007325</td>\n",
       "      <td>0.486135</td>\n",
       "      <td>0.655900</td>\n",
       "      <td>0.422702</td>\n",
       "      <td>0.474513</td>\n",
       "      <td>0.761400</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.370812</td>\n",
       "      <td>0.367436</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.800495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.694556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057812</td>\n",
       "      <td>0.408672</td>\n",
       "      <td>0.460508</td>\n",
       "      <td>0.434589</td>\n",
       "      <td>0.885521</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>3.278019e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.751278</td>\n",
       "      <td>0.885081</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.908094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020657</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130382</td>\n",
       "      <td>0.242172</td>\n",
       "      <td>0.525080</td>\n",
       "      <td>0.331882</td>\n",
       "      <td>0.994919</td>\n",
       "      <td>1.865208e-16</td>\n",
       "      <td>0.892785</td>\n",
       "      <td>5.409271e-01</td>\n",
       "      <td>0.445944</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.464863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18449</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.432141</td>\n",
       "      <td>0.796603</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.801648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073024</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330049</td>\n",
       "      <td>0.212908</td>\n",
       "      <td>0.513742</td>\n",
       "      <td>0.363772</td>\n",
       "      <td>0.907713</td>\n",
       "      <td>1.464996e-01</td>\n",
       "      <td>0.297476</td>\n",
       "      <td>4.474913e-01</td>\n",
       "      <td>0.452203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.450298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.981829</td>\n",
       "      <td>0.596258</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.789105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175971</td>\n",
       "      <td>0.067841</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.207411</td>\n",
       "      <td>0.652872</td>\n",
       "      <td>0.524922</td>\n",
       "      <td>0.469930</td>\n",
       "      <td>0.919265</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.892785</td>\n",
       "      <td>5.409271e-01</td>\n",
       "      <td>0.445944</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.464863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.408291</td>\n",
       "      <td>0.532188</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.782552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.720766</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.662963</td>\n",
       "      <td>0.594033</td>\n",
       "      <td>0.327858</td>\n",
       "      <td>0.896508</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>2.922838e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18452</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.833049</td>\n",
       "      <td>0.660327</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.825778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045668</td>\n",
       "      <td>0.144152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063488</td>\n",
       "      <td>0.817354</td>\n",
       "      <td>0.409319</td>\n",
       "      <td>0.395123</td>\n",
       "      <td>0.852558</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>2.922838e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18453</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.642249</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.795405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.218749</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.110111</td>\n",
       "      <td>0.222999</td>\n",
       "      <td>0.440477</td>\n",
       "      <td>0.485818</td>\n",
       "      <td>0.818032</td>\n",
       "      <td>3.878689e-01</td>\n",
       "      <td>0.053253</td>\n",
       "      <td>9.561615e-01</td>\n",
       "      <td>0.300293</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.057691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18454 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track_popularity  danceability    energy       key  loudness  mode  \\\n",
       "0                  0.41      0.662692  0.390827  0.181818  0.681000   1.0   \n",
       "1                  0.28      0.232254  0.877962  0.818182  0.830868   1.0   \n",
       "2                  0.00      0.847814  0.646090  0.545455  0.753108   0.0   \n",
       "3                  0.41      0.370812  0.367436  0.454545  0.800495   0.0   \n",
       "4                  0.65      0.751278  0.885081  0.818182  0.908094   1.0   \n",
       "...                 ...           ...       ...       ...       ...   ...   \n",
       "18449              0.00      0.432141  0.796603  0.909091  0.801648   0.0   \n",
       "18450              0.49      0.981829  0.596258  0.181818  0.789105   1.0   \n",
       "18451              0.40      0.408291  0.532188  0.454545  0.782552   0.0   \n",
       "18452              0.36      0.833049  0.660327  0.090909  0.825778   0.0   \n",
       "18453              0.61      0.642249  0.529137  0.181818  0.795405   1.0   \n",
       "\n",
       "       speechiness  acousticness  instrumentalness  liveness   valence  \\\n",
       "0         0.001340      0.281249          0.011854  0.080414  0.571136   \n",
       "1         0.024341      0.011793          0.010071  0.342212  0.407663   \n",
       "2         0.216168      0.004353          0.007325  0.486135  0.655900   \n",
       "3         0.013064      0.694556          0.000000  0.057812  0.408672   \n",
       "4         0.020657      0.037297          0.000000  0.130382  0.242172   \n",
       "...            ...           ...               ...       ...       ...   \n",
       "18449     0.073024      0.005593          0.000000  0.330049  0.212908   \n",
       "18450     0.175971      0.067841          0.000006  0.207411  0.652872   \n",
       "18451     0.005136      0.720766          0.000434  0.107070  0.662963   \n",
       "18452     0.045668      0.144152          0.000000  0.063488  0.817354   \n",
       "18453     0.007369      0.218749          0.000003  0.110111  0.222999   \n",
       "\n",
       "          tempo  duration_ms  days_since_release         qual0     qual1  \\\n",
       "0      0.338981     0.418893            0.697950  3.878689e-01  0.053253   \n",
       "1      0.554509     0.703040            0.965821  3.277147e-02  0.000000   \n",
       "2      0.422702     0.474513            0.761400  6.037235e-02  0.140397   \n",
       "3      0.460508     0.434589            0.885521  6.037235e-02  0.140397   \n",
       "4      0.525080     0.331882            0.994919  1.865208e-16  0.892785   \n",
       "...         ...          ...                 ...           ...       ...   \n",
       "18449  0.513742     0.363772            0.907713  1.464996e-01  0.297476   \n",
       "18450  0.524922     0.469930            0.919265  0.000000e+00  0.892785   \n",
       "18451  0.594033     0.327858            0.896508  6.037235e-02  0.140397   \n",
       "18452  0.409319     0.395123            0.852558  6.037235e-02  0.140397   \n",
       "18453  0.440477     0.485818            0.818032  3.878689e-01  0.053253   \n",
       "\n",
       "              qual2     qual3     qual4     qual5  \n",
       "0      9.561615e-01  0.300293  0.097080  0.057691  \n",
       "1      9.137683e-01  0.383860  0.143633  0.440446  \n",
       "2      0.000000e+00  0.203910  0.123639  0.443461  \n",
       "3      3.278019e-14  0.203910  0.123639  0.443461  \n",
       "4      5.409271e-01  0.445944  0.179673  0.464863  \n",
       "...             ...       ...       ...       ...  \n",
       "18449  4.474913e-01  0.452203  1.000000  0.450298  \n",
       "18450  5.409271e-01  0.445944  0.179673  0.464863  \n",
       "18451  2.922838e-14  0.203910  0.123639  0.443461  \n",
       "18452  2.922838e-14  0.203910  0.123639  0.443461  \n",
       "18453  9.561615e-01  0.300293  0.097080  0.057691  \n",
       "\n",
       "[18454 rows x 20 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnum  = dfnum.drop(['playlist_genre','language'],axis=1)\n",
    "dfnum  =pd.concat([dfnum,dfqual],axis=1)\n",
    "dfnum  = dfnum.astype(float)\n",
    "dfnum   = NormalizeData(dfnum) #scaling\n",
    "dfnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>days_since_release</th>\n",
       "      <th>qual0</th>\n",
       "      <th>qual1</th>\n",
       "      <th>qual2</th>\n",
       "      <th>qual3</th>\n",
       "      <th>qual4</th>\n",
       "      <th>qual5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.662692</td>\n",
       "      <td>0.390827</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.281249</td>\n",
       "      <td>0.011854</td>\n",
       "      <td>0.080414</td>\n",
       "      <td>0.571136</td>\n",
       "      <td>0.338981</td>\n",
       "      <td>0.418893</td>\n",
       "      <td>0.697950</td>\n",
       "      <td>3.878689e-01</td>\n",
       "      <td>0.053253</td>\n",
       "      <td>9.561615e-01</td>\n",
       "      <td>0.300293</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.057691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.232254</td>\n",
       "      <td>0.877962</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.830868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024341</td>\n",
       "      <td>0.011793</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>0.342212</td>\n",
       "      <td>0.407663</td>\n",
       "      <td>0.554509</td>\n",
       "      <td>0.703040</td>\n",
       "      <td>0.965821</td>\n",
       "      <td>3.277147e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.137683e-01</td>\n",
       "      <td>0.383860</td>\n",
       "      <td>0.143633</td>\n",
       "      <td>0.440446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.370812</td>\n",
       "      <td>0.367436</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.800495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.694556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057812</td>\n",
       "      <td>0.408672</td>\n",
       "      <td>0.460508</td>\n",
       "      <td>0.434589</td>\n",
       "      <td>0.885521</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>3.278019e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.751278</td>\n",
       "      <td>0.885081</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.908094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020657</td>\n",
       "      <td>0.037297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130382</td>\n",
       "      <td>0.242172</td>\n",
       "      <td>0.525080</td>\n",
       "      <td>0.331882</td>\n",
       "      <td>0.994919</td>\n",
       "      <td>1.865208e-16</td>\n",
       "      <td>0.892785</td>\n",
       "      <td>5.409271e-01</td>\n",
       "      <td>0.445944</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.464863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.451448</td>\n",
       "      <td>0.632869</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.790989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.282257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.307763</td>\n",
       "      <td>0.625378</td>\n",
       "      <td>0.396831</td>\n",
       "      <td>0.988448</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>2.922838e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18448</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.733106</td>\n",
       "      <td>0.710160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.792508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.366905</td>\n",
       "      <td>0.101813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083658</td>\n",
       "      <td>0.343081</td>\n",
       "      <td>0.666060</td>\n",
       "      <td>0.304332</td>\n",
       "      <td>0.954095</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>2.922838e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18450</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.981829</td>\n",
       "      <td>0.596258</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.789105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.175971</td>\n",
       "      <td>0.067841</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.207411</td>\n",
       "      <td>0.652872</td>\n",
       "      <td>0.524922</td>\n",
       "      <td>0.469930</td>\n",
       "      <td>0.919265</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.892785</td>\n",
       "      <td>5.409271e-01</td>\n",
       "      <td>0.445944</td>\n",
       "      <td>0.179673</td>\n",
       "      <td>0.464863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18451</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.408291</td>\n",
       "      <td>0.532188</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.782552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.720766</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.107070</td>\n",
       "      <td>0.662963</td>\n",
       "      <td>0.594033</td>\n",
       "      <td>0.327858</td>\n",
       "      <td>0.896508</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>2.922838e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18452</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.833049</td>\n",
       "      <td>0.660327</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.825778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045668</td>\n",
       "      <td>0.144152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063488</td>\n",
       "      <td>0.817354</td>\n",
       "      <td>0.409319</td>\n",
       "      <td>0.395123</td>\n",
       "      <td>0.852558</td>\n",
       "      <td>6.037235e-02</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>2.922838e-14</td>\n",
       "      <td>0.203910</td>\n",
       "      <td>0.123639</td>\n",
       "      <td>0.443461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18453</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.642249</td>\n",
       "      <td>0.529137</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.795405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.218749</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.110111</td>\n",
       "      <td>0.222999</td>\n",
       "      <td>0.440477</td>\n",
       "      <td>0.485818</td>\n",
       "      <td>0.818032</td>\n",
       "      <td>3.878689e-01</td>\n",
       "      <td>0.053253</td>\n",
       "      <td>9.561615e-01</td>\n",
       "      <td>0.300293</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.057691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15640 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track_popularity  danceability    energy       key  loudness  mode  \\\n",
       "0                  0.41      0.662692  0.390827  0.181818  0.681000   1.0   \n",
       "1                  0.28      0.232254  0.877962  0.818182  0.830868   1.0   \n",
       "3                  0.41      0.370812  0.367436  0.454545  0.800495   0.0   \n",
       "4                  0.65      0.751278  0.885081  0.818182  0.908094   1.0   \n",
       "5                  0.70      0.451448  0.632869  0.545455  0.790989   1.0   \n",
       "...                 ...           ...       ...       ...       ...   ...   \n",
       "18448              0.72      0.733106  0.710160  0.000000  0.792508   1.0   \n",
       "18450              0.49      0.981829  0.596258  0.181818  0.789105   1.0   \n",
       "18451              0.40      0.408291  0.532188  0.454545  0.782552   0.0   \n",
       "18452              0.36      0.833049  0.660327  0.090909  0.825778   0.0   \n",
       "18453              0.61      0.642249  0.529137  0.181818  0.795405   1.0   \n",
       "\n",
       "       speechiness  acousticness  instrumentalness  liveness   valence  \\\n",
       "0         0.001340      0.281249          0.011854  0.080414  0.571136   \n",
       "1         0.024341      0.011793          0.010071  0.342212  0.407663   \n",
       "3         0.013064      0.694556          0.000000  0.057812  0.408672   \n",
       "4         0.020657      0.037297          0.000000  0.130382  0.242172   \n",
       "5         0.036400      0.282257          0.000000  0.089333  0.307763   \n",
       "...            ...           ...               ...       ...       ...   \n",
       "18448     0.366905      0.101813          0.000000  0.083658  0.343081   \n",
       "18450     0.175971      0.067841          0.000006  0.207411  0.652872   \n",
       "18451     0.005136      0.720766          0.000434  0.107070  0.662963   \n",
       "18452     0.045668      0.144152          0.000000  0.063488  0.817354   \n",
       "18453     0.007369      0.218749          0.000003  0.110111  0.222999   \n",
       "\n",
       "          tempo  duration_ms  days_since_release         qual0     qual1  \\\n",
       "0      0.338981     0.418893            0.697950  3.878689e-01  0.053253   \n",
       "1      0.554509     0.703040            0.965821  3.277147e-02  0.000000   \n",
       "3      0.460508     0.434589            0.885521  6.037235e-02  0.140397   \n",
       "4      0.525080     0.331882            0.994919  1.865208e-16  0.892785   \n",
       "5      0.625378     0.396831            0.988448  6.037235e-02  0.140397   \n",
       "...         ...          ...                 ...           ...       ...   \n",
       "18448  0.666060     0.304332            0.954095  6.037235e-02  0.140397   \n",
       "18450  0.524922     0.469930            0.919265  0.000000e+00  0.892785   \n",
       "18451  0.594033     0.327858            0.896508  6.037235e-02  0.140397   \n",
       "18452  0.409319     0.395123            0.852558  6.037235e-02  0.140397   \n",
       "18453  0.440477     0.485818            0.818032  3.878689e-01  0.053253   \n",
       "\n",
       "              qual2     qual3     qual4     qual5  \n",
       "0      9.561615e-01  0.300293  0.097080  0.057691  \n",
       "1      9.137683e-01  0.383860  0.143633  0.440446  \n",
       "3      3.278019e-14  0.203910  0.123639  0.443461  \n",
       "4      5.409271e-01  0.445944  0.179673  0.464863  \n",
       "5      2.922838e-14  0.203910  0.123639  0.443461  \n",
       "...             ...       ...       ...       ...  \n",
       "18448  2.922838e-14  0.203910  0.123639  0.443461  \n",
       "18450  5.409271e-01  0.445944  0.179673  0.464863  \n",
       "18451  2.922838e-14  0.203910  0.123639  0.443461  \n",
       "18452  2.922838e-14  0.203910  0.123639  0.443461  \n",
       "18453  9.561615e-01  0.300293  0.097080  0.057691  \n",
       "\n",
       "[15640 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfump = dfnum[dfnum.track_popularity <= 0.05]\n",
    "dfnum = dfnum[dfnum.track_popularity > 0.05]\n",
    "dfnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8SdM2PL_IAd"
   },
   "source": [
    "Now our data is clean and ready to be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMoptdFg_QM6"
   },
   "source": [
    "# Data mining and exploration\n",
    "\n",
    "In this section, I we will search for relations and patters hidden in the data to understand what are we looking for, before applying the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x212a66ea2c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(dfnum.loc[:,'track_popularity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UyumjQfgAFC2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamaño de los targets:  (15640,)\n",
      "tamaño de los features:  (15640, 19)\n",
      "[0.41 0.28 0.41 ... 0.4  0.36 0.61]\n"
     ]
    }
   ],
   "source": [
    "# Separa features y targest\n",
    "targets = dfnum.pop('track_popularity')\n",
    "targets = np.array(targets, dtype = 'float64')\n",
    "targets.reshape((targets.shape[0],1))\n",
    "#\n",
    "features = np.array(dfnum,dtype = 'float64' )\n",
    "#\n",
    "print(\"tamaño de los targets: \",targets.shape)\n",
    "print(\"tamaño de los features: \",features.shape)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  8,  9, ...,  9,  9, -1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's see if we can sepate the data into\n",
    "#clusters in order to apply the model separately:\n",
    "dbscan = cluster.DBSCAN(min_samples=100)\n",
    "dbscan.fit(features)\n",
    "sns.histplot(dbscan.labels_)\n",
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that clustering didn't work (meaning that there is no clear way to separate it in groups with similar features). Therefore, we are going to model it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Jz7wmCxIDWsh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training samples: (12512, 19)\n",
      "Shape of validation samples: (3128, 19)\n",
      "[0.23623753 0.18732462 0.16783144 0.15981959 0.10957926 0.06521964]\n",
      "0.9260120863101037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x212a70970c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features, val_features, train_targets, val_targets = train_test_split(features, targets, test_size=num_val_samples, shuffle=True)\n",
    "\n",
    "print(\"Shape of training samples:\", np.shape(train_features))\n",
    "print(\"Shape of validation samples:\", np.shape(val_features))\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharey=False)\n",
    "fig.suptitle('Initial Pokemon - 1st Generation')\n",
    "sns.despine(left=True)\n",
    "sns.histplot(ax=axes[0],x=train_targets)\n",
    "sns.histplot(ax=axes[1],x=val_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJXG9ARs_twF"
   },
   "source": [
    "# Building our predicition model\n",
    "\n",
    "Now that our data is clear and understood, we are ready for the good part! Builing our Maching Leaning models. We are going to try two approaches: Neural Networks and Reggresion Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(Model): #the NN approach\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.dense1   = Dense(32, activation='leaky_relu')\n",
    "        self.dropout1 = Dropout(.2, input_shape=(32,))\n",
    "        self.dense2   = Dense(256, activation='leaky_relu') #si 56: 64,32,8,1\n",
    "        self.dropout2 = Dropout(.2, input_shape=(256,)) \n",
    "        self.dense3   = Dense(512, activation='leaky_relu')\n",
    "        self.dropout3 = Dropout(.2,input_shape=(512,))\n",
    "        self.dense4   = Dense(256, activation='leaky_relu')\n",
    "        self.dropout4 = Dropout(.2, input_shape=(256,))\n",
    "        self.dense5   = Dense(128, activation='leaky_relu')\n",
    "        self.dropout5 = Dropout(.2, input_shape=(128,))\n",
    "        self.dense6   = Dense(16, activation='leaky_relu')\n",
    "        self.dropout6 = Dropout(.2, input_shape=(16,))\n",
    "        self.dense7   = Dense(8, activation='leaky_relu')\n",
    "        self.dropout7 = Dropout(.2, input_shape=(8,))\n",
    "        self.dense8   = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):                    \n",
    "        x = self.dense1(inputs)                \n",
    "        x = self.dropout1(x)                   \n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.dense6(x)\n",
    "        x = self.dropout6(x)\n",
    "        x = self.dense7(x)\n",
    "        x = self.dropout7(x)\n",
    "        return self.dense8(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spotify = CustomModel(name='my_custom_model')\n",
    "model_spotify.compile(optimizer=Adam(5e-5),\n",
    "              loss='mse',\n",
    "              metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "63/63 [==============================] - 4s 24ms/step - loss: 0.0350 - mae: 0.1519 - val_loss: 0.0350 - val_mae: 0.1520\n",
      "Epoch 2/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0348 - mae: 0.1517 - val_loss: 0.0348 - val_mae: 0.1515\n",
      "Epoch 3/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0347 - mae: 0.1511 - val_loss: 0.0345 - val_mae: 0.1507\n",
      "Epoch 4/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0345 - mae: 0.1507 - val_loss: 0.0342 - val_mae: 0.1501\n",
      "Epoch 5/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0344 - mae: 0.1502 - val_loss: 0.0342 - val_mae: 0.1492\n",
      "Epoch 6/1000\n",
      "63/63 [==============================] - 2s 30ms/step - loss: 0.0343 - mae: 0.1501 - val_loss: 0.0340 - val_mae: 0.1493\n",
      "Epoch 7/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0342 - mae: 0.1498 - val_loss: 0.0339 - val_mae: 0.1493\n",
      "Epoch 8/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0342 - mae: 0.1495 - val_loss: 0.0339 - val_mae: 0.1492\n",
      "Epoch 9/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0342 - mae: 0.1498 - val_loss: 0.0339 - val_mae: 0.1489\n",
      "Epoch 10/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0341 - mae: 0.1495 - val_loss: 0.0338 - val_mae: 0.1486\n",
      "Epoch 11/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0341 - mae: 0.1496 - val_loss: 0.0338 - val_mae: 0.1482\n",
      "Epoch 12/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0340 - mae: 0.1491 - val_loss: 0.0336 - val_mae: 0.1482\n",
      "Epoch 13/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0341 - mae: 0.1491 - val_loss: 0.0337 - val_mae: 0.1484\n",
      "Epoch 14/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0340 - mae: 0.1493 - val_loss: 0.0337 - val_mae: 0.1482\n",
      "Epoch 15/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0338 - mae: 0.1487 - val_loss: 0.0335 - val_mae: 0.1478\n",
      "Epoch 16/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0339 - mae: 0.1489 - val_loss: 0.0335 - val_mae: 0.1478\n",
      "Epoch 17/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0340 - mae: 0.1491 - val_loss: 0.0335 - val_mae: 0.1478\n",
      "Epoch 18/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0340 - mae: 0.1489 - val_loss: 0.0335 - val_mae: 0.1478\n",
      "Epoch 19/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0339 - mae: 0.1489 - val_loss: 0.0335 - val_mae: 0.1481\n",
      "Epoch 20/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0338 - mae: 0.1489 - val_loss: 0.0334 - val_mae: 0.1474\n",
      "Epoch 21/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0338 - mae: 0.1486 - val_loss: 0.0333 - val_mae: 0.1478\n",
      "Epoch 22/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0337 - mae: 0.1482 - val_loss: 0.0334 - val_mae: 0.1476\n",
      "Epoch 23/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0337 - mae: 0.1485 - val_loss: 0.0333 - val_mae: 0.1472\n",
      "Epoch 24/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0337 - mae: 0.1485 - val_loss: 0.0333 - val_mae: 0.1474\n",
      "Epoch 25/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0337 - mae: 0.1484 - val_loss: 0.0333 - val_mae: 0.1474\n",
      "Epoch 26/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0338 - mae: 0.1485 - val_loss: 0.0333 - val_mae: 0.1476\n",
      "Epoch 27/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0337 - mae: 0.1485 - val_loss: 0.0333 - val_mae: 0.1472\n",
      "Epoch 28/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0337 - mae: 0.1483 - val_loss: 0.0332 - val_mae: 0.1471\n",
      "Epoch 29/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0337 - mae: 0.1482 - val_loss: 0.0332 - val_mae: 0.1474\n",
      "Epoch 30/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0336 - mae: 0.1481 - val_loss: 0.0332 - val_mae: 0.1472\n",
      "Epoch 31/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0337 - mae: 0.1482 - val_loss: 0.0333 - val_mae: 0.1471\n",
      "Epoch 32/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0336 - mae: 0.1480 - val_loss: 0.0332 - val_mae: 0.1472\n",
      "Epoch 33/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0335 - mae: 0.1477 - val_loss: 0.0331 - val_mae: 0.1471\n",
      "Epoch 34/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0335 - mae: 0.1476 - val_loss: 0.0331 - val_mae: 0.1469\n",
      "Epoch 35/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0336 - mae: 0.1479 - val_loss: 0.0332 - val_mae: 0.1472\n",
      "Epoch 36/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0336 - mae: 0.1481 - val_loss: 0.0331 - val_mae: 0.1473\n",
      "Epoch 37/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0336 - mae: 0.1479 - val_loss: 0.0331 - val_mae: 0.1472\n",
      "Epoch 38/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0336 - mae: 0.1480 - val_loss: 0.0331 - val_mae: 0.1473\n",
      "Epoch 39/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0334 - mae: 0.1477 - val_loss: 0.0331 - val_mae: 0.1468\n",
      "Epoch 40/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0334 - mae: 0.1475 - val_loss: 0.0330 - val_mae: 0.1467\n",
      "Epoch 41/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0334 - mae: 0.1474 - val_loss: 0.0330 - val_mae: 0.1468\n",
      "Epoch 42/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0334 - mae: 0.1475 - val_loss: 0.0330 - val_mae: 0.1469\n",
      "Epoch 43/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0335 - mae: 0.1480 - val_loss: 0.0331 - val_mae: 0.1468\n",
      "Epoch 44/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0336 - mae: 0.1477 - val_loss: 0.0330 - val_mae: 0.1469\n",
      "Epoch 45/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0332 - mae: 0.1473 - val_loss: 0.0329 - val_mae: 0.1463\n",
      "Epoch 46/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0334 - mae: 0.1476 - val_loss: 0.0330 - val_mae: 0.1468\n",
      "Epoch 47/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0334 - mae: 0.1476 - val_loss: 0.0330 - val_mae: 0.1467\n",
      "Epoch 48/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0334 - mae: 0.1476 - val_loss: 0.0329 - val_mae: 0.1464\n",
      "Epoch 49/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0334 - mae: 0.1477 - val_loss: 0.0330 - val_mae: 0.1467\n",
      "Epoch 50/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0334 - mae: 0.1475 - val_loss: 0.0329 - val_mae: 0.1467\n",
      "Epoch 51/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0334 - mae: 0.1475 - val_loss: 0.0329 - val_mae: 0.1466\n",
      "Epoch 52/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1472 - val_loss: 0.0329 - val_mae: 0.1465\n",
      "Epoch 53/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0333 - mae: 0.1476 - val_loss: 0.0329 - val_mae: 0.1466\n",
      "Epoch 54/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0333 - mae: 0.1472 - val_loss: 0.0329 - val_mae: 0.1464\n",
      "Epoch 55/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0334 - mae: 0.1474 - val_loss: 0.0329 - val_mae: 0.1462\n",
      "Epoch 56/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0333 - mae: 0.1470 - val_loss: 0.0329 - val_mae: 0.1464\n",
      "Epoch 57/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0334 - mae: 0.1475 - val_loss: 0.0329 - val_mae: 0.1466\n",
      "Epoch 58/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0332 - mae: 0.1472 - val_loss: 0.0329 - val_mae: 0.1466\n",
      "Epoch 59/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0334 - mae: 0.1474 - val_loss: 0.0329 - val_mae: 0.1466\n",
      "Epoch 60/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1471 - val_loss: 0.0328 - val_mae: 0.1463\n",
      "Epoch 61/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0333 - mae: 0.1473 - val_loss: 0.0329 - val_mae: 0.1465\n",
      "Epoch 62/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0333 - mae: 0.1472 - val_loss: 0.0328 - val_mae: 0.1463\n",
      "Epoch 63/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0334 - mae: 0.1475 - val_loss: 0.0329 - val_mae: 0.1467\n",
      "Epoch 64/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0333 - mae: 0.1472 - val_loss: 0.0329 - val_mae: 0.1462\n",
      "Epoch 65/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0333 - mae: 0.1473 - val_loss: 0.0328 - val_mae: 0.1462\n",
      "Epoch 66/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0333 - mae: 0.1472 - val_loss: 0.0328 - val_mae: 0.1462\n",
      "Epoch 67/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1469 - val_loss: 0.0328 - val_mae: 0.1464\n",
      "Epoch 68/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1469 - val_loss: 0.0328 - val_mae: 0.1464\n",
      "Epoch 69/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0331 - mae: 0.1468 - val_loss: 0.0328 - val_mae: 0.1460\n",
      "Epoch 70/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1467 - val_loss: 0.0328 - val_mae: 0.1458\n",
      "Epoch 71/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0331 - mae: 0.1466 - val_loss: 0.0327 - val_mae: 0.1460\n",
      "Epoch 72/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0331 - mae: 0.1467 - val_loss: 0.0327 - val_mae: 0.1460\n",
      "Epoch 73/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0333 - mae: 0.1473 - val_loss: 0.0327 - val_mae: 0.1462\n",
      "Epoch 74/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0333 - mae: 0.1471 - val_loss: 0.0328 - val_mae: 0.1461\n",
      "Epoch 75/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0330 - mae: 0.1467 - val_loss: 0.0327 - val_mae: 0.1458\n",
      "Epoch 76/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1468 - val_loss: 0.0327 - val_mae: 0.1459\n",
      "Epoch 77/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1470 - val_loss: 0.0327 - val_mae: 0.1458\n",
      "Epoch 78/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0332 - mae: 0.1470 - val_loss: 0.0328 - val_mae: 0.1463\n",
      "Epoch 79/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1470 - val_loss: 0.0327 - val_mae: 0.1460\n",
      "Epoch 80/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1467 - val_loss: 0.0328 - val_mae: 0.1464\n",
      "Epoch 81/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0331 - mae: 0.1470 - val_loss: 0.0327 - val_mae: 0.1462\n",
      "Epoch 82/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0331 - mae: 0.1468 - val_loss: 0.0327 - val_mae: 0.1461\n",
      "Epoch 83/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1467 - val_loss: 0.0326 - val_mae: 0.1457\n",
      "Epoch 84/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0331 - mae: 0.1465 - val_loss: 0.0327 - val_mae: 0.1461\n",
      "Epoch 85/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1466 - val_loss: 0.0326 - val_mae: 0.1457\n",
      "Epoch 86/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0330 - mae: 0.1466 - val_loss: 0.0326 - val_mae: 0.1458\n",
      "Epoch 87/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0331 - mae: 0.1466 - val_loss: 0.0326 - val_mae: 0.1459\n",
      "Epoch 88/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0331 - mae: 0.1466 - val_loss: 0.0326 - val_mae: 0.1459\n",
      "Epoch 89/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0332 - mae: 0.1472 - val_loss: 0.0327 - val_mae: 0.1460\n",
      "Epoch 90/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0332 - mae: 0.1467 - val_loss: 0.0327 - val_mae: 0.1458\n",
      "Epoch 91/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0330 - mae: 0.1465 - val_loss: 0.0326 - val_mae: 0.1455\n",
      "Epoch 92/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1464 - val_loss: 0.0326 - val_mae: 0.1458\n",
      "Epoch 93/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0331 - mae: 0.1466 - val_loss: 0.0326 - val_mae: 0.1458\n",
      "Epoch 94/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1467 - val_loss: 0.0326 - val_mae: 0.1456\n",
      "Epoch 95/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1460 - val_loss: 0.0325 - val_mae: 0.1453\n",
      "Epoch 96/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1464 - val_loss: 0.0326 - val_mae: 0.1458\n",
      "Epoch 97/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1465 - val_loss: 0.0326 - val_mae: 0.1457\n",
      "Epoch 98/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1465 - val_loss: 0.0326 - val_mae: 0.1459\n",
      "Epoch 99/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0330 - mae: 0.1466 - val_loss: 0.0326 - val_mae: 0.1456\n",
      "Epoch 100/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1464 - val_loss: 0.0326 - val_mae: 0.1456\n",
      "Epoch 101/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1463 - val_loss: 0.0326 - val_mae: 0.1455\n",
      "Epoch 102/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0330 - mae: 0.1463 - val_loss: 0.0326 - val_mae: 0.1458\n",
      "Epoch 103/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0329 - mae: 0.1463 - val_loss: 0.0326 - val_mae: 0.1454\n",
      "Epoch 104/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1464 - val_loss: 0.0326 - val_mae: 0.1454\n",
      "Epoch 105/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1465 - val_loss: 0.0326 - val_mae: 0.1456\n",
      "Epoch 106/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1458 - val_loss: 0.0325 - val_mae: 0.1455\n",
      "Epoch 107/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1464 - val_loss: 0.0326 - val_mae: 0.1459\n",
      "Epoch 108/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0328 - mae: 0.1464 - val_loss: 0.0325 - val_mae: 0.1457\n",
      "Epoch 109/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0329 - mae: 0.1460 - val_loss: 0.0325 - val_mae: 0.1456\n",
      "Epoch 110/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0330 - mae: 0.1462 - val_loss: 0.0325 - val_mae: 0.1453\n",
      "Epoch 111/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0329 - mae: 0.1462 - val_loss: 0.0325 - val_mae: 0.1454\n",
      "Epoch 112/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0329 - mae: 0.1463 - val_loss: 0.0325 - val_mae: 0.1453\n",
      "Epoch 113/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0329 - mae: 0.1460 - val_loss: 0.0325 - val_mae: 0.1456\n",
      "Epoch 114/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0329 - mae: 0.1463 - val_loss: 0.0325 - val_mae: 0.1455\n",
      "Epoch 115/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0329 - mae: 0.1459 - val_loss: 0.0325 - val_mae: 0.1454\n",
      "Epoch 116/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0329 - mae: 0.1459 - val_loss: 0.0325 - val_mae: 0.1455\n",
      "Epoch 117/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0328 - mae: 0.1459 - val_loss: 0.0326 - val_mae: 0.1460\n",
      "Epoch 118/1000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0330 - mae: 0.146 - 1s 20ms/step - loss: 0.0330 - mae: 0.1468 - val_loss: 0.0325 - val_mae: 0.1455\n",
      "Epoch 119/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1456 - val_loss: 0.0325 - val_mae: 0.1455\n",
      "Epoch 120/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0327 - mae: 0.1460 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 121/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0329 - mae: 0.1457 - val_loss: 0.0325 - val_mae: 0.1458\n",
      "Epoch 122/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0329 - mae: 0.1461 - val_loss: 0.0325 - val_mae: 0.1454\n",
      "Epoch 123/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0328 - mae: 0.1458 - val_loss: 0.0325 - val_mae: 0.1456\n",
      "Epoch 124/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1458 - val_loss: 0.0325 - val_mae: 0.1456\n",
      "Epoch 125/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1455 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 126/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0329 - mae: 0.1458 - val_loss: 0.0325 - val_mae: 0.1453\n",
      "Epoch 127/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0328 - mae: 0.1457 - val_loss: 0.0324 - val_mae: 0.1453\n",
      "Epoch 128/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1458 - val_loss: 0.0324 - val_mae: 0.1454\n",
      "Epoch 129/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1457 - val_loss: 0.0324 - val_mae: 0.1453\n",
      "Epoch 130/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1457 - val_loss: 0.0324 - val_mae: 0.1451\n",
      "Epoch 131/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0328 - mae: 0.1459 - val_loss: 0.0325 - val_mae: 0.1452\n",
      "Epoch 132/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1457 - val_loss: 0.0325 - val_mae: 0.1459\n",
      "Epoch 133/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1460 - val_loss: 0.0325 - val_mae: 0.1455\n",
      "Epoch 134/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1458 - val_loss: 0.0324 - val_mae: 0.1451\n",
      "Epoch 135/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1457 - val_loss: 0.0324 - val_mae: 0.1454\n",
      "Epoch 136/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0327 - mae: 0.1458 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 137/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1458 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 138/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1454 - val_loss: 0.0325 - val_mae: 0.1458\n",
      "Epoch 139/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1456 - val_loss: 0.0324 - val_mae: 0.1449\n",
      "Epoch 140/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0326 - mae: 0.1455 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 141/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0326 - mae: 0.1458 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 142/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1455 - val_loss: 0.0324 - val_mae: 0.1453\n",
      "Epoch 143/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1457 - val_loss: 0.0324 - val_mae: 0.1451\n",
      "Epoch 144/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1458 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 145/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1452 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 146/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1455 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 147/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0326 - mae: 0.1453 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 148/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0328 - mae: 0.1457 - val_loss: 0.0324 - val_mae: 0.1455\n",
      "Epoch 149/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1455 - val_loss: 0.0324 - val_mae: 0.1453\n",
      "Epoch 150/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1454 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 151/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1451 - val_loss: 0.0324 - val_mae: 0.1451\n",
      "Epoch 152/1000\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.0327 - mae: 0.1457 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 153/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0326 - mae: 0.1452 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 154/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0328 - mae: 0.1456 - val_loss: 0.0324 - val_mae: 0.1455\n",
      "Epoch 155/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0326 - mae: 0.1455 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 156/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0325 - mae: 0.1451 - val_loss: 0.0323 - val_mae: 0.1449\n",
      "Epoch 157/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0325 - mae: 0.1453 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 158/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0325 - mae: 0.1451 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 159/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0325 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1451\n",
      "Epoch 160/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1454 - val_loss: 0.0323 - val_mae: 0.1451\n",
      "Epoch 161/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0325 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 162/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0327 - mae: 0.1454 - val_loss: 0.0323 - val_mae: 0.1453\n",
      "Epoch 163/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0326 - mae: 0.1454 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 164/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0326 - mae: 0.1452 - val_loss: 0.0324 - val_mae: 0.1455\n",
      "Epoch 165/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0326 - mae: 0.1452 - val_loss: 0.0323 - val_mae: 0.1447\n",
      "Epoch 166/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1448 - val_loss: 0.0323 - val_mae: 0.1449\n",
      "Epoch 167/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1454 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 168/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0324 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 169/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0326 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 170/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0327 - mae: 0.1455 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 171/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0326 - mae: 0.1453 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 172/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0324 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 173/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0325 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 174/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0325 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 175/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0325 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1452\n",
      "Epoch 176/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0325 - mae: 0.1450 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 177/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0324 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 178/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1451 - val_loss: 0.0322 - val_mae: 0.1448\n",
      "Epoch 179/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0325 - mae: 0.1451 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 180/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0325 - mae: 0.1447 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 181/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0324 - mae: 0.1446 - val_loss: 0.0322 - val_mae: 0.1448\n",
      "Epoch 182/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1449 - val_loss: 0.0322 - val_mae: 0.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0324 - mae: 0.1444 - val_loss: 0.0322 - val_mae: 0.1448\n",
      "Epoch 184/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1447 - val_loss: 0.0322 - val_mae: 0.1449\n",
      "Epoch 185/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0324 - mae: 0.1450 - val_loss: 0.0322 - val_mae: 0.1449\n",
      "Epoch 186/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0325 - mae: 0.1448 - val_loss: 0.0323 - val_mae: 0.1454\n",
      "Epoch 187/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0325 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1448\n",
      "Epoch 188/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1447 - val_loss: 0.0322 - val_mae: 0.1450\n",
      "Epoch 189/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1450 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 190/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1446 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 191/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0324 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1449\n",
      "Epoch 192/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0325 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 193/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1447 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 194/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1449 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 195/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1445 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 196/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0323 - mae: 0.1445 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 197/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1443 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 198/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 199/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0324 - mae: 0.1450 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 200/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1447 - val_loss: 0.0323 - val_mae: 0.1450\n",
      "Epoch 201/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1448 - val_loss: 0.0322 - val_mae: 0.1452\n",
      "Epoch 202/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1445 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 203/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0324 - mae: 0.1447 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 204/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1445 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 205/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 206/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 207/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 208/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1446 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 209/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0323 - mae: 0.1446 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 210/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0324 - mae: 0.1450 - val_loss: 0.0322 - val_mae: 0.1448\n",
      "Epoch 211/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1442 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 212/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1445 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 213/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1442 - val_loss: 0.0322 - val_mae: 0.1451\n",
      "Epoch 214/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 215/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1445 - val_loss: 0.0322 - val_mae: 0.1450\n",
      "Epoch 216/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 217/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1443 - val_loss: 0.0322 - val_mae: 0.1449\n",
      "Epoch 218/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0323 - mae: 0.1449 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 219/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1443 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 220/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 221/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1442 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 222/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1444 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 223/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1442 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 224/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0322 - mae: 0.1439 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 225/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1443 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 226/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 227/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1440 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 228/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1443 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 229/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0323 - mae: 0.1440 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 230/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1442 - val_loss: 0.0320 - val_mae: 0.1445\n",
      "Epoch 231/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1445 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 232/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1443 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 233/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 234/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0322 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 235/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 236/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1444 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 237/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0322 - mae: 0.1440 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 238/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0321 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 239/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0321 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 240/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1438 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 241/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1437 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 242/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1440 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 243/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0322 - mae: 0.1442 - val_loss: 0.0321 - val_mae: 0.1444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0321 - mae: 0.1440 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 245/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 246/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1432 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 247/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0322 - mae: 0.1443 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 248/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 249/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0322 - mae: 0.1441 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 250/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1442 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 251/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 252/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 253/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1436 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 254/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1440 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 255/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1441 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 256/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0321 - mae: 0.1442 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 257/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 258/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0320 - mae: 0.1435 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 259/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1440 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 260/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1440 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 261/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 262/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0320 - mae: 0.1442 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 263/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0320 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 264/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1443 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 265/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1436 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 266/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1436 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 267/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0321 - mae: 0.1440 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 268/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 269/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 270/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1440 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 271/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 272/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1435 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 273/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1433 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 274/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1436 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 275/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1437 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 276/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 277/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1437 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 278/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 279/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1436 - val_loss: 0.0321 - val_mae: 0.1448\n",
      "Epoch 280/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1435 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 281/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 282/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1437 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 283/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1438 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 284/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0319 - mae: 0.1436 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 285/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1435 - val_loss: 0.0322 - val_mae: 0.1449\n",
      "Epoch 286/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0321 - mae: 0.1433 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 287/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1439 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 288/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0321 - mae: 0.1438 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 289/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1439 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 290/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0322 - mae: 0.1439 - val_loss: 0.0321 - val_mae: 0.1449\n",
      "Epoch 291/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1435 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 292/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0318 - mae: 0.1430 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 293/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0321 - val_mae: 0.1450\n",
      "Epoch 294/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1438 - val_loss: 0.0320 - val_mae: 0.1445\n",
      "Epoch 295/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1437 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 296/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 297/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0321 - mae: 0.1438 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 298/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0318 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 299/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 300/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1434 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 301/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1433 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 302/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1438 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 303/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 304/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0320 - mae: 0.1434 - val_loss: 0.0320 - val_mae: 0.1441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0320 - mae: 0.1435 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 306/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1437 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 307/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0318 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 308/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1435 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 309/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 310/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 311/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1433 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 312/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1433 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 313/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 314/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0320 - mae: 0.1437 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 315/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1437 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 316/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 317/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 318/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1433 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 319/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 320/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1430 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 321/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 322/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0318 - mae: 0.1433 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 323/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 324/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 325/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 326/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1433 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 327/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 328/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 329/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1433 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 330/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 331/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0320 - mae: 0.1435 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 332/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 333/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 334/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 335/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1432 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 336/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 337/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 338/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 339/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1425 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 340/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 341/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 342/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1426 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 343/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0319 - mae: 0.1433 - val_loss: 0.0320 - val_mae: 0.1445\n",
      "Epoch 344/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 345/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 346/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 347/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 348/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 349/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1427 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 350/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 351/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 352/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1434 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 353/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 354/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 355/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0318 - mae: 0.1431 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 356/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0317 - mae: 0.1427 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 357/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1434\n",
      "Epoch 358/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1428 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 359/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 360/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1423 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 361/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0319 - val_mae: 0.1440\n",
      "Epoch 362/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 363/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 364/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 365/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1425 - val_loss: 0.0320 - val_mae: 0.1437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0319 - mae: 0.1429 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 367/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 368/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0318 - mae: 0.1430 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 369/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1445\n",
      "Epoch 370/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 371/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 372/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1427 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 373/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1425 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 374/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 375/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1425 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 376/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1427 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 377/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0318 - mae: 0.1429 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 378/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 379/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 380/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0317 - mae: 0.1427 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 381/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 382/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 383/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 384/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1426 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 385/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1426 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 386/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0318 - mae: 0.1432 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 387/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1421 - val_loss: 0.0318 - val_mae: 0.1433\n",
      "Epoch 388/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1423 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 389/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 390/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1428 - val_loss: 0.0318 - val_mae: 0.1432\n",
      "Epoch 391/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 392/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1425 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 393/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1440\n",
      "Epoch 394/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 395/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 396/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1422 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 397/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0315 - mae: 0.1423 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 398/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1422 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 399/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 400/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1427 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 401/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1428 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 402/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1440\n",
      "Epoch 403/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1430 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 404/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1426 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 405/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 406/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 407/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1423 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 408/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1422 - val_loss: 0.0319 - val_mae: 0.1441\n",
      "Epoch 409/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 410/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 411/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0317 - mae: 0.1426 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 412/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1426 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 413/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 414/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 415/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 416/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0317 - mae: 0.1427 - val_loss: 0.0319 - val_mae: 0.1434\n",
      "Epoch 417/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1425 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 418/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 419/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 420/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 421/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1425 - val_loss: 0.0319 - val_mae: 0.1432\n",
      "Epoch 422/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 423/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 424/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 425/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 426/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0316 - mae: 0.1422 - val_loss: 0.0320 - val_mae: 0.1439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0315 - mae: 0.1419 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 428/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 429/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1425 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 430/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 431/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1423 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 432/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1423 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 433/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0314 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 434/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0315 - mae: 0.1423 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 435/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0316 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 436/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 437/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 438/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1434\n",
      "Epoch 439/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0314 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 440/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1424 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 441/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 442/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0315 - mae: 0.1419 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 443/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0313 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 444/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 445/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1422 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 446/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0314 - mae: 0.1420 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 447/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1417 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 448/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1423 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 449/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 450/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1444\n",
      "Epoch 451/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 452/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 453/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1419 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 454/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 455/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 456/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 457/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 458/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0316 - mae: 0.1426 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 459/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1419 - val_loss: 0.0319 - val_mae: 0.1439\n",
      "Epoch 460/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1420 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 461/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 462/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 463/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 464/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1415 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 465/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 466/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 467/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 468/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 469/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1417 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 470/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 471/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 472/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 473/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1415 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 474/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 475/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 476/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 477/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1417 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 478/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 479/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 480/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 481/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1416 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 482/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 483/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 484/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 485/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0314 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 486/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 487/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0319 - val_mae: 0.1436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 489/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1416 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 490/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 491/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0314 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 492/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0320 - val_mae: 0.1443\n",
      "Epoch 493/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 494/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1409 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 495/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 496/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 497/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 498/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 499/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1421 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 500/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0314 - mae: 0.1417 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 501/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0315 - mae: 0.1420 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 502/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 503/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1417 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 504/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0315 - mae: 0.1422 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 505/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 506/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1417 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 507/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 508/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0311 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 509/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1410 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 510/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.0319 - val_mae: 0.1434\n",
      "Epoch 511/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 512/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1419 - val_loss: 0.0319 - val_mae: 0.1438\n",
      "Epoch 513/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 514/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1415 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 515/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 516/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 517/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0322 - val_mae: 0.1448\n",
      "Epoch 518/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 519/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1421 - val_loss: 0.0319 - val_mae: 0.1437\n",
      "Epoch 520/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 521/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 522/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 523/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 524/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 525/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1417 - val_loss: 0.0319 - val_mae: 0.1433\n",
      "Epoch 526/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 527/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 528/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1414 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 529/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 530/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1419 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 531/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1410 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 532/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 533/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 534/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 535/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 536/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1412 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 537/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 538/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 539/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0314 - mae: 0.1418 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 540/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1413 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 541/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0312 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 542/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0321 - val_mae: 0.1447\n",
      "Epoch 543/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1417 - val_loss: 0.0319 - val_mae: 0.1433\n",
      "Epoch 544/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 545/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 546/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 547/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1409 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 548/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 549/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 550/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 551/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 552/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 553/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0319 - val_mae: 0.1436\n",
      "Epoch 554/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1418 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 555/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 556/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 557/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 558/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 559/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1408 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 560/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 561/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 562/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0313 - mae: 0.1415 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 563/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1409 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 564/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1407 - val_loss: 0.0319 - val_mae: 0.1434\n",
      "Epoch 565/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1410 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 566/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1409 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 567/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 568/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1409 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 569/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 570/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 571/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 572/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1446\n",
      "Epoch 573/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1409 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 574/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 575/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1441\n",
      "Epoch 576/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0312 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1442\n",
      "Epoch 577/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 578/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 579/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 580/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0319 - val_mae: 0.1432\n",
      "Epoch 581/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0314 - mae: 0.1413 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 582/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0313 - mae: 0.1414 - val_loss: 0.0321 - val_mae: 0.1445\n",
      "Epoch 583/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1410 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 584/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 585/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 586/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 587/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 588/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 589/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1412 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 590/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 591/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 592/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 593/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 594/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 595/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1408 - val_loss: 0.0320 - val_mae: 0.1440\n",
      "Epoch 596/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 597/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 598/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 599/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1409 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 600/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1409 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 601/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 602/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 603/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1412 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 604/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 605/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 606/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1409 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 607/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 608/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 609/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 610/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0309 - mae: 0.1403 - val_loss: 0.0323 - val_mae: 0.1451\n",
      "Epoch 611/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 612/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1409 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 613/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1414 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 614/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0319 - val_mae: 0.1434\n",
      "Epoch 615/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1408 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 616/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 617/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1408 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 618/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0313 - mae: 0.1416 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 619/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 620/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1409 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 621/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1406 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 622/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 623/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1410 - val_loss: 0.0319 - val_mae: 0.1435\n",
      "Epoch 624/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 625/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 626/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1411 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 627/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 628/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0312 - mae: 0.1413 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 629/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 630/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0308 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 631/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 632/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1406 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 633/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 634/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 635/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0307 - mae: 0.1402 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 636/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 637/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 638/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1411 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 639/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 640/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 641/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 642/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 643/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1411 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 644/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 645/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0312 - mae: 0.1409 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 646/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 647/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1405 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 648/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 649/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 650/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 651/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 652/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0311 - mae: 0.1412 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 653/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 654/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0311 - mae: 0.1410 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 655/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 656/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 657/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0308 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 658/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 659/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1447\n",
      "Epoch 660/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 661/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 662/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1407 - val_loss: 0.0321 - val_mae: 0.1444\n",
      "Epoch 663/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 664/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0320 - val_mae: 0.1438\n",
      "Epoch 665/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1407 - val_loss: 0.0320 - val_mae: 0.1439\n",
      "Epoch 666/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 667/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 668/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0309 - mae: 0.1403 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 669/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 670/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0323 - val_mae: 0.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 671/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0307 - mae: 0.1400 - val_loss: 0.0320 - val_mae: 0.1434\n",
      "Epoch 672/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 673/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0309 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 674/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0309 - mae: 0.1408 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 675/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0310 - mae: 0.1405 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 676/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1443\n",
      "Epoch 677/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0309 - mae: 0.1403 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 678/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0310 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 679/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 680/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0320 - val_mae: 0.1437\n",
      "Epoch 681/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0308 - mae: 0.1400 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 682/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0310 - mae: 0.1407 - val_loss: 0.0320 - val_mae: 0.1433\n",
      "Epoch 683/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0309 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1438\n",
      "Epoch 684/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 685/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 686/1000\n",
      "63/63 [==============================] - 2s 30ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 687/1000\n",
      "63/63 [==============================] - 2s 33ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 688/1000\n",
      "63/63 [==============================] - 2s 30ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 689/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 690/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0308 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 691/1000\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 692/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 693/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 694/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0308 - mae: 0.1404 - val_loss: 0.0320 - val_mae: 0.1436\n",
      "Epoch 695/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 696/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 697/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0309 - mae: 0.1407 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 698/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 699/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 700/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 701/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 702/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0310 - mae: 0.1406 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 703/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 704/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0308 - mae: 0.1400 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 705/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 706/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 707/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 708/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0307 - mae: 0.1402 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 709/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 710/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0308 - mae: 0.1404 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 711/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0323 - val_mae: 0.1447\n",
      "Epoch 712/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 713/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0308 - mae: 0.1399 - val_loss: 0.0321 - val_mae: 0.1442\n",
      "Epoch 714/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 715/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 716/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0307 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1436\n",
      "Epoch 717/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0309 - mae: 0.1404 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 718/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0309 - mae: 0.1405 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 719/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 720/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0309 - mae: 0.1406 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 721/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 722/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0308 - mae: 0.1405 - val_loss: 0.0321 - val_mae: 0.1437\n",
      "Epoch 723/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0308 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 724/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0308 - mae: 0.1400 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 725/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0308 - mae: 0.1399 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 726/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1400 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 727/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 728/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 729/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1397 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 730/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1400 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 731/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0321 - val_mae: 0.1440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 732/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 733/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1438\n",
      "Epoch 734/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 735/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 736/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 737/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 738/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 739/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0309 - mae: 0.1402 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 740/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 741/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 742/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1393 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 743/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 744/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1400 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 745/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 746/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0321 - val_mae: 0.1438\n",
      "Epoch 747/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 748/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 749/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 750/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 751/1000\n",
      "63/63 [==============================] - 1s 18ms/step - loss: 0.0308 - mae: 0.1400 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 752/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 753/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 754/1000\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 0.0305 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 755/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0307 - mae: 0.1403 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 756/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 757/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1447\n",
      "Epoch 758/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 759/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0305 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 760/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 761/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0308 - mae: 0.1401 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 762/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 763/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 764/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 765/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 766/1000\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.0307 - mae: 0.1394 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 767/1000\n",
      "63/63 [==============================] - 2s 32ms/step - loss: 0.0306 - mae: 0.1396 - val_loss: 0.0324 - val_mae: 0.1452\n",
      "Epoch 768/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0308 - mae: 0.1402 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 769/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0306 - mae: 0.1399 - val_loss: 0.0321 - val_mae: 0.1438\n",
      "Epoch 770/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0308 - mae: 0.1399 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 771/1000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0307 - mae: 0.1398- ETA: 0s - loss: 0.0306 - ma - 2s 24ms/step - loss: 0.0307 - mae: 0.1400 - val_loss: 0.0323 - val_mae: 0.1447\n",
      "Epoch 772/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 773/1000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0308 - mae: 0.140 - 2s 29ms/step - loss: 0.0308 - mae: 0.1403 - val_loss: 0.0321 - val_mae: 0.1441\n",
      "Epoch 774/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 775/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0321 - val_mae: 0.1436\n",
      "Epoch 776/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 777/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0305 - mae: 0.1397 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 778/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 779/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1400 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 780/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 781/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0305 - mae: 0.1398 - val_loss: 0.0321 - val_mae: 0.1436\n",
      "Epoch 782/1000\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 0.0307 - mae: 0.1396 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 783/1000\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 784/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 785/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0305 - mae: 0.1394 - val_loss: 0.0321 - val_mae: 0.1440\n",
      "Epoch 786/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0307 - mae: 0.1401 - val_loss: 0.0321 - val_mae: 0.1435\n",
      "Epoch 787/1000\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0322 - val_mae: 0.1442ss: 0.0303 - ma\n",
      "Epoch 788/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 789/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 790/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 791/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 792/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1400 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 793/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 794/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 795/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1387 - val_loss: 0.0321 - val_mae: 0.1435\n",
      "Epoch 796/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0321 - val_mae: 0.1436\n",
      "Epoch 797/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1446\n",
      "Epoch 798/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 799/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1391 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 800/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 801/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 802/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1396 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 803/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 804/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1397 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 805/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 806/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 807/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 808/1000\n",
      "63/63 [==============================] - 2s 29ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 809/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0322 - val_mae: 0.1444\n",
      "Epoch 810/1000\n",
      "63/63 [==============================] - 2s 33ms/step - loss: 0.0305 - mae: 0.1391 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 811/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1387 - val_loss: 0.0321 - val_mae: 0.1439\n",
      "Epoch 812/1000\n",
      "63/63 [==============================] - 2s 28ms/step - loss: 0.0307 - mae: 0.1397 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 813/1000\n",
      "63/63 [==============================] - 2s 32ms/step - loss: 0.0308 - mae: 0.1399 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 814/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0306 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 815/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 816/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 817/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1397 - val_loss: 0.0323 - val_mae: 0.1447\n",
      "Epoch 818/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 819/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 820/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0305 - mae: 0.1394 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 821/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0305 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 822/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0321 - val_mae: 0.1436\n",
      "Epoch 823/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 824/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 825/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1445\n",
      "Epoch 826/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 827/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 828/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 829/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1392 - val_loss: 0.0322 - val_mae: 0.1438\n",
      "Epoch 830/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 831/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 832/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1398 - val_loss: 0.0321 - val_mae: 0.1436\n",
      "Epoch 833/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 834/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1392 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 835/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1443\n",
      "Epoch 836/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 837/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0322 - val_mae: 0.1441\n",
      "Epoch 838/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 839/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 840/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 841/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 842/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1396 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 843/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0323 - val_mae: 0.1441\n",
      "Epoch 844/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1396 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 845/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 846/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1399 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 847/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0323 - val_mae: 0.1442\n",
      "Epoch 848/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 849/1000\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 0.0305 - mae: 0.1390 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 850/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 851/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1449\n",
      "Epoch 852/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0324 - val_mae: 0.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 853/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 854/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1387 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 855/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0323 - val_mae: 0.1439\n",
      "Epoch 856/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1397 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 857/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1393 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 858/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 859/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1393 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 860/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1440\n",
      "Epoch 861/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1389 - val_loss: 0.0323 - val_mae: 0.1446ss: 0.0305 - mae: 0.1\n",
      "Epoch 862/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 863/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0307 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 864/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1389 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 865/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1390 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 866/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0325 - val_mae: 0.1449\n",
      "Epoch 867/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 868/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0325 - val_mae: 0.1452\n",
      "Epoch 869/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1396 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 870/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1392 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 871/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1394 - val_loss: 0.0323 - val_mae: 0.1442\n",
      "Epoch 872/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0325 - val_mae: 0.1456.0303 - mae: 0.13\n",
      "Epoch 873/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0306 - mae: 0.1398 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 874/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1448\n",
      "Epoch 875/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 876/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1387 - val_loss: 0.0324 - val_mae: 0.1449\n",
      "Epoch 877/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0322 - val_mae: 0.1438\n",
      "Epoch 878/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 879/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1392 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 880/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 881/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1386 - val_loss: 0.0322 - val_mae: 0.1442\n",
      "Epoch 882/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1393 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 883/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1393 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 884/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1381 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 885/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1393 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 886/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1390 - val_loss: 0.0325 - val_mae: 0.1452\n",
      "Epoch 887/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1395 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 888/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0306 - mae: 0.1395 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 889/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1389 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 890/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1389 - val_loss: 0.0323 - val_mae: 0.1441\n",
      "Epoch 891/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 892/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 893/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1385 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 894/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 895/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0322 - val_mae: 0.1438\n",
      "Epoch 896/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1389 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 897/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1387 - val_loss: 0.0324 - val_mae: 0.1449\n",
      "Epoch 898/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1392 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 899/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0322 - val_mae: 0.1439\n",
      "Epoch 900/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0322 - val_mae: 0.1438\n",
      "Epoch 901/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1384 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 902/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1389 - val_loss: 0.0322 - val_mae: 0.1438\n",
      "Epoch 903/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1384 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 904/1000\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 0.0303 - mae: 0.1390 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 905/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1392 - val_loss: 0.0323 - val_mae: 0.1440\n",
      "Epoch 906/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1386 - val_loss: 0.0324 - val_mae: 0.1451\n",
      "Epoch 907/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 908/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1386 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 909/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1387 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 910/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1382 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 911/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1389 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 912/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 913/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1387 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 914/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 915/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0324 - val_mae: 0.1451\n",
      "Epoch 916/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0325 - val_mae: 0.1451\n",
      "Epoch 917/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 918/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0301 - mae: 0.1386 - val_loss: 0.0323 - val_mae: 0.1442\n",
      "Epoch 919/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 920/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1390 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 921/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1390 - val_loss: 0.0322 - val_mae: 0.1436\n",
      "Epoch 922/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0323 - val_mae: 0.1442\n",
      "Epoch 923/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 924/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 925/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1394 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 926/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1385 - val_loss: 0.0324 - val_mae: 0.1443\n",
      "Epoch 927/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1389 - val_loss: 0.0323 - val_mae: 0.1442\n",
      "Epoch 928/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 929/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1396 - val_loss: 0.0322 - val_mae: 0.1440\n",
      "Epoch 930/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1385 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 931/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0325 - val_mae: 0.1448\n",
      "Epoch 932/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1387 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 933/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1384 - val_loss: 0.0325 - val_mae: 0.1452\n",
      "Epoch 934/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1386 - val_loss: 0.0326 - val_mae: 0.1453\n",
      "Epoch 935/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 936/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 937/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1389 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 938/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 939/1000\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0305 - mae: 0.1388- ETA: 0s - loss: 0.0310 -  - 1s 23ms/step - loss: 0.0305 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1446\n",
      "Epoch 940/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1386 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 941/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 942/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 943/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0301 - mae: 0.1379 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 944/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 945/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 946/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 947/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0323 - val_mae: 0.1444\n",
      "Epoch 948/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1385 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 949/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1387 - val_loss: 0.0326 - val_mae: 0.1454\n",
      "Epoch 950/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 951/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1385 - val_loss: 0.0325 - val_mae: 0.1449\n",
      "Epoch 952/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1379 - val_loss: 0.0325 - val_mae: 0.1447\n",
      "Epoch 953/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1387 - val_loss: 0.0325 - val_mae: 0.1447\n",
      "Epoch 954/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1383 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 955/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0326 - val_mae: 0.1451\n",
      "Epoch 956/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0325 - val_mae: 0.1451\n",
      "Epoch 957/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0301 - mae: 0.1386 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 958/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0300 - mae: 0.1381 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 959/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 960/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1389 - val_loss: 0.0326 - val_mae: 0.1457\n",
      "Epoch 961/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1389 - val_loss: 0.0323 - val_mae: 0.1445\n",
      "Epoch 962/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1386 - val_loss: 0.0325 - val_mae: 0.1449\n",
      "Epoch 963/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1384 - val_loss: 0.0325 - val_mae: 0.1448\n",
      "Epoch 964/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 965/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0304 - mae: 0.1391 - val_loss: 0.0324 - val_mae: 0.1443\n",
      "Epoch 966/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1385 - val_loss: 0.0325 - val_mae: 0.1447\n",
      "Epoch 967/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1443\n",
      "Epoch 968/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1382 - val_loss: 0.0325 - val_mae: 0.1448\n",
      "Epoch 969/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1384 - val_loss: 0.0325 - val_mae: 0.1446\n",
      "Epoch 970/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1387 - val_loss: 0.0325 - val_mae: 0.1451\n",
      "Epoch 971/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0302 - mae: 0.1384 - val_loss: 0.0325 - val_mae: 0.1446\n",
      "Epoch 972/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1386 - val_loss: 0.0323 - val_mae: 0.1440\n",
      "Epoch 973/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0304 - mae: 0.1386 - val_loss: 0.0325 - val_mae: 0.1448\n",
      "Epoch 974/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0303 - mae: 0.1391 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 975/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1391 - val_loss: 0.0323 - val_mae: 0.1443\n",
      "Epoch 976/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1381 - val_loss: 0.0325 - val_mae: 0.1449\n",
      "Epoch 977/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1383 - val_loss: 0.0325 - val_mae: 0.1446\n",
      "Epoch 978/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0305 - mae: 0.1392 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 979/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1448\n",
      "Epoch 980/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1385 - val_loss: 0.0324 - val_mae: 0.1446\n",
      "Epoch 981/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0300 - mae: 0.1381 - val_loss: 0.0325 - val_mae: 0.1447\n",
      "Epoch 982/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1381 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 983/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0301 - mae: 0.1388 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 984/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0300 - mae: 0.1382 - val_loss: 0.0324 - val_mae: 0.1450\n",
      "Epoch 985/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0301 - mae: 0.1383 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 986/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1389 - val_loss: 0.0324 - val_mae: 0.1443\n",
      "Epoch 987/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0300 - mae: 0.1380 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 988/1000\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 0.0302 - mae: 0.1383 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 989/1000\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 0.0300 - mae: 0.1379 - val_loss: 0.0323 - val_mae: 0.1441\n",
      "Epoch 990/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0303 - mae: 0.1386 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 991/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0301 - mae: 0.1385 - val_loss: 0.0324 - val_mae: 0.1445\n",
      "Epoch 992/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1387 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 993/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1387 - val_loss: 0.0326 - val_mae: 0.1456\n",
      "Epoch 994/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0300 - mae: 0.1382 - val_loss: 0.0324 - val_mae: 0.1447\n",
      "Epoch 995/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0300 - mae: 0.1378 - val_loss: 0.0324 - val_mae: 0.14430s - loss: 0.0299 - mae:\n",
      "Epoch 996/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0300 - mae: 0.1380 - val_loss: 0.0325 - val_mae: 0.1450\n",
      "Epoch 997/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0300 - mae: 0.1382 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 998/1000\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 0.0302 - mae: 0.1384 - val_loss: 0.0324 - val_mae: 0.1444\n",
      "Epoch 999/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0301 - mae: 0.1379 - val_loss: 0.0325 - val_mae: 0.1449\n",
      "Epoch 1000/1000\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 0.0300 - mae: 0.1381 - val_loss: 0.0324 - val_mae: 0.1442\n"
     ]
    }
   ],
   "source": [
    "history = model_spotify.fit(\n",
    "    train_features,\n",
    "    train_targets,\n",
    "    batch_size=200,\n",
    "    epochs=1000,\n",
    "    verbose=1,\n",
    "    validation_data=(val_features, val_targets)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 0s 3ms/step - loss: 0.0324 - mae: 0.1442\n",
      "[0.03241853788495064, 0.1441660076379776]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'mae']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model_spotify.evaluate(val_features, val_targets)\n",
    "print(score)\n",
    "model_spotify.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.29066708]\n",
      " [0.29066708 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "y_prediction = model_spotify.predict(val_features)\n",
    "y_prediction = np.squeeze(y_prediction)\n",
    "plt.scatter(val_targets, y_prediction)\n",
    "plt.xlabel('Targets')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "my_rho = np.corrcoef(val_targets, y_prediction)\n",
    "print(my_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x212b04494c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results are not quite good. The corelation coefficient is too low and the distribution of the predicted values does not match the targets. Let's try the other approach and see if this situation gets better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DJIzHCd-DnXx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 1000\n",
      "building tree 3 of 1000\n",
      "building tree 4 of 1000\n",
      "building tree 5 of 1000\n",
      "building tree 6 of 1000\n",
      "building tree 7 of 1000\n",
      "building tree 8 of 1000\n",
      "building tree 9 of 1000\n",
      "building tree 10 of 1000\n",
      "building tree 11 of 1000\n",
      "building tree 12 of 1000\n",
      "building tree 13 of 1000\n",
      "building tree 14 of 1000\n",
      "building tree 15 of 1000\n",
      "building tree 16 of 1000\n",
      "building tree 17 of 1000\n",
      "building tree 18 of 1000\n",
      "building tree 19 of 1000\n",
      "building tree 20 of 1000\n",
      "building tree 21 of 1000\n",
      "building tree 22 of 1000\n",
      "building tree 23 of 1000\n",
      "building tree 24 of 1000\n",
      "building tree 25 of 1000\n",
      "building tree 26 of 1000\n",
      "building tree 27 of 1000\n",
      "building tree 28 of 1000\n",
      "building tree 29 of 1000\n",
      "building tree 30 of 1000\n",
      "building tree 31 of 1000\n",
      "building tree 32 of 1000\n",
      "building tree 33 of 1000\n",
      "building tree 34 of 1000\n",
      "building tree 35 of 1000\n",
      "building tree 36 of 1000\n",
      "building tree 37 of 1000\n",
      "building tree 38 of 1000\n",
      "building tree 39 of 1000\n",
      "building tree 40 of 1000\n",
      "building tree 41 of 1000\n",
      "building tree 42 of 1000\n",
      "building tree 43 of 1000\n",
      "building tree 44 of 1000\n",
      "building tree 45 of 1000\n",
      "building tree 46 of 1000\n",
      "building tree 47 of 1000\n",
      "building tree 48 of 1000\n",
      "building tree 49 of 1000\n",
      "building tree 50 of 1000\n",
      "building tree 51 of 1000\n",
      "building tree 52 of 1000\n",
      "building tree 53 of 1000\n",
      "building tree 54 of 1000\n",
      "building tree 55 of 1000\n",
      "building tree 56 of 1000\n",
      "building tree 57 of 1000\n",
      "building tree 58 of 1000\n",
      "building tree 59 of 1000\n",
      "building tree 60 of 1000\n",
      "building tree 61 of 1000\n",
      "building tree 62 of 1000\n",
      "building tree 63 of 1000\n",
      "building tree 64 of 1000\n",
      "building tree 65 of 1000\n",
      "building tree 66 of 1000\n",
      "building tree 67 of 1000\n",
      "building tree 68 of 1000\n",
      "building tree 69 of 1000\n",
      "building tree 70 of 1000\n",
      "building tree 71 of 1000\n",
      "building tree 72 of 1000\n",
      "building tree 73 of 1000\n",
      "building tree 74 of 1000\n",
      "building tree 75 of 1000\n",
      "building tree 76 of 1000\n",
      "building tree 77 of 1000\n",
      "building tree 78 of 1000\n",
      "building tree 79 of 1000\n",
      "building tree 80 of 1000\n",
      "building tree 81 of 1000\n",
      "building tree 82 of 1000\n",
      "building tree 83 of 1000\n",
      "building tree 84 of 1000\n",
      "building tree 85 of 1000\n",
      "building tree 86 of 1000\n",
      "building tree 87 of 1000\n",
      "building tree 88 of 1000\n",
      "building tree 89 of 1000\n",
      "building tree 90 of 1000\n",
      "building tree 91 of 1000\n",
      "building tree 92 of 1000\n",
      "building tree 93 of 1000\n",
      "building tree 94 of 1000\n",
      "building tree 95 of 1000\n",
      "building tree 96 of 1000\n",
      "building tree 97 of 1000\n",
      "building tree 98 of 1000\n",
      "building tree 99 of 1000\n",
      "building tree 100 of 1000\n",
      "building tree 101 of 1000\n",
      "building tree 102 of 1000\n",
      "building tree 103 of 1000\n",
      "building tree 104 of 1000\n",
      "building tree 105 of 1000\n",
      "building tree 106 of 1000\n",
      "building tree 107 of 1000\n",
      "building tree 108 of 1000\n",
      "building tree 109 of 1000\n",
      "building tree 110 of 1000\n",
      "building tree 111 of 1000\n",
      "building tree 112 of 1000\n",
      "building tree 113 of 1000\n",
      "building tree 114 of 1000\n",
      "building tree 115 of 1000\n",
      "building tree 116 of 1000\n",
      "building tree 117 of 1000\n",
      "building tree 118 of 1000\n",
      "building tree 119 of 1000\n",
      "building tree 120 of 1000\n",
      "building tree 121 of 1000\n",
      "building tree 122 of 1000\n",
      "building tree 123 of 1000\n",
      "building tree 124 of 1000\n",
      "building tree 125 of 1000\n",
      "building tree 126 of 1000\n",
      "building tree 127 of 1000\n",
      "building tree 128 of 1000\n",
      "building tree 129 of 1000\n",
      "building tree 130 of 1000\n",
      "building tree 131 of 1000\n",
      "building tree 132 of 1000\n",
      "building tree 133 of 1000\n",
      "building tree 134 of 1000\n",
      "building tree 135 of 1000\n",
      "building tree 136 of 1000\n",
      "building tree 137 of 1000\n",
      "building tree 138 of 1000\n",
      "building tree 139 of 1000\n",
      "building tree 140 of 1000\n",
      "building tree 141 of 1000\n",
      "building tree 142 of 1000\n",
      "building tree 143 of 1000\n",
      "building tree 144 of 1000\n",
      "building tree 145 of 1000\n",
      "building tree 146 of 1000\n",
      "building tree 147 of 1000\n",
      "building tree 148 of 1000\n",
      "building tree 149 of 1000\n",
      "building tree 150 of 1000\n",
      "building tree 151 of 1000\n",
      "building tree 152 of 1000\n",
      "building tree 153 of 1000\n",
      "building tree 154 of 1000\n",
      "building tree 155 of 1000\n",
      "building tree 156 of 1000\n",
      "building tree 157 of 1000\n",
      "building tree 158 of 1000\n",
      "building tree 159 of 1000\n",
      "building tree 160 of 1000\n",
      "building tree 161 of 1000\n",
      "building tree 162 of 1000\n",
      "building tree 163 of 1000\n",
      "building tree 164 of 1000\n",
      "building tree 165 of 1000\n",
      "building tree 166 of 1000\n",
      "building tree 167 of 1000\n",
      "building tree 168 of 1000\n",
      "building tree 169 of 1000\n",
      "building tree 170 of 1000\n",
      "building tree 171 of 1000\n",
      "building tree 172 of 1000\n",
      "building tree 173 of 1000\n",
      "building tree 174 of 1000\n",
      "building tree 175 of 1000\n",
      "building tree 176 of 1000\n",
      "building tree 177 of 1000\n",
      "building tree 178 of 1000\n",
      "building tree 179 of 1000\n",
      "building tree 180 of 1000\n",
      "building tree 181 of 1000\n",
      "building tree 182 of 1000\n",
      "building tree 183 of 1000\n",
      "building tree 184 of 1000\n",
      "building tree 185 of 1000\n",
      "building tree 186 of 1000\n",
      "building tree 187 of 1000\n",
      "building tree 188 of 1000\n",
      "building tree 189 of 1000\n",
      "building tree 190 of 1000\n",
      "building tree 191 of 1000\n",
      "building tree 192 of 1000\n",
      "building tree 193 of 1000\n",
      "building tree 194 of 1000\n",
      "building tree 195 of 1000\n",
      "building tree 196 of 1000\n",
      "building tree 197 of 1000\n",
      "building tree 198 of 1000\n",
      "building tree 199 of 1000\n",
      "building tree 200 of 1000\n",
      "building tree 201 of 1000\n",
      "building tree 202 of 1000\n",
      "building tree 203 of 1000\n",
      "building tree 204 of 1000\n",
      "building tree 205 of 1000\n",
      "building tree 206 of 1000\n",
      "building tree 207 of 1000\n",
      "building tree 208 of 1000\n",
      "building tree 209 of 1000\n",
      "building tree 210 of 1000\n",
      "building tree 211 of 1000\n",
      "building tree 212 of 1000\n",
      "building tree 213 of 1000\n",
      "building tree 214 of 1000\n",
      "building tree 215 of 1000\n",
      "building tree 216 of 1000\n",
      "building tree 217 of 1000\n",
      "building tree 218 of 1000\n",
      "building tree 219 of 1000\n",
      "building tree 220 of 1000\n",
      "building tree 221 of 1000\n",
      "building tree 222 of 1000\n",
      "building tree 223 of 1000\n",
      "building tree 224 of 1000\n",
      "building tree 225 of 1000\n",
      "building tree 226 of 1000\n",
      "building tree 227 of 1000\n",
      "building tree 228 of 1000\n",
      "building tree 229 of 1000\n",
      "building tree 230 of 1000\n",
      "building tree 231 of 1000\n",
      "building tree 232 of 1000\n",
      "building tree 233 of 1000\n",
      "building tree 234 of 1000\n",
      "building tree 235 of 1000\n",
      "building tree 236 of 1000\n",
      "building tree 237 of 1000\n",
      "building tree 238 of 1000\n",
      "building tree 239 of 1000\n",
      "building tree 240 of 1000\n",
      "building tree 241 of 1000\n",
      "building tree 242 of 1000\n",
      "building tree 243 of 1000\n",
      "building tree 244 of 1000\n",
      "building tree 245 of 1000\n",
      "building tree 246 of 1000\n",
      "building tree 247 of 1000\n",
      "building tree 248 of 1000\n",
      "building tree 249 of 1000\n",
      "building tree 250 of 1000\n",
      "building tree 251 of 1000\n",
      "building tree 252 of 1000\n",
      "building tree 253 of 1000\n",
      "building tree 254 of 1000\n",
      "building tree 255 of 1000\n",
      "building tree 256 of 1000\n",
      "building tree 257 of 1000\n",
      "building tree 258 of 1000\n",
      "building tree 259 of 1000\n",
      "building tree 260 of 1000\n",
      "building tree 261 of 1000\n",
      "building tree 262 of 1000\n",
      "building tree 263 of 1000\n",
      "building tree 264 of 1000\n",
      "building tree 265 of 1000\n",
      "building tree 266 of 1000\n",
      "building tree 267 of 1000\n",
      "building tree 268 of 1000\n",
      "building tree 269 of 1000\n",
      "building tree 270 of 1000\n",
      "building tree 271 of 1000\n",
      "building tree 272 of 1000\n",
      "building tree 273 of 1000\n",
      "building tree 274 of 1000\n",
      "building tree 275 of 1000\n",
      "building tree 276 of 1000\n",
      "building tree 277 of 1000\n",
      "building tree 278 of 1000\n",
      "building tree 279 of 1000\n",
      "building tree 280 of 1000\n",
      "building tree 281 of 1000\n",
      "building tree 282 of 1000\n",
      "building tree 283 of 1000\n",
      "building tree 284 of 1000\n",
      "building tree 285 of 1000\n",
      "building tree 286 of 1000\n",
      "building tree 287 of 1000\n",
      "building tree 288 of 1000\n",
      "building tree 289 of 1000\n",
      "building tree 290 of 1000\n",
      "building tree 291 of 1000\n",
      "building tree 292 of 1000\n",
      "building tree 293 of 1000\n",
      "building tree 294 of 1000\n",
      "building tree 295 of 1000\n",
      "building tree 296 of 1000\n",
      "building tree 297 of 1000\n",
      "building tree 298 of 1000\n",
      "building tree 299 of 1000\n",
      "building tree 300 of 1000\n",
      "building tree 301 of 1000\n",
      "building tree 302 of 1000\n",
      "building tree 303 of 1000\n",
      "building tree 304 of 1000\n",
      "building tree 305 of 1000\n",
      "building tree 306 of 1000\n",
      "building tree 307 of 1000\n",
      "building tree 308 of 1000\n",
      "building tree 309 of 1000\n",
      "building tree 310 of 1000\n",
      "building tree 311 of 1000\n",
      "building tree 312 of 1000\n",
      "building tree 313 of 1000\n",
      "building tree 314 of 1000\n",
      "building tree 315 of 1000\n",
      "building tree 316 of 1000\n",
      "building tree 317 of 1000\n",
      "building tree 318 of 1000\n",
      "building tree 319 of 1000\n",
      "building tree 320 of 1000\n",
      "building tree 321 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 322 of 1000\n",
      "building tree 323 of 1000\n",
      "building tree 324 of 1000\n",
      "building tree 325 of 1000\n",
      "building tree 326 of 1000\n",
      "building tree 327 of 1000\n",
      "building tree 328 of 1000\n",
      "building tree 329 of 1000\n",
      "building tree 330 of 1000\n",
      "building tree 331 of 1000\n",
      "building tree 332 of 1000\n",
      "building tree 333 of 1000\n",
      "building tree 334 of 1000\n",
      "building tree 335 of 1000\n",
      "building tree 336 of 1000\n",
      "building tree 337 of 1000\n",
      "building tree 338 of 1000\n",
      "building tree 339 of 1000\n",
      "building tree 340 of 1000\n",
      "building tree 341 of 1000\n",
      "building tree 342 of 1000\n",
      "building tree 343 of 1000\n",
      "building tree 344 of 1000\n",
      "building tree 345 of 1000\n",
      "building tree 346 of 1000\n",
      "building tree 347 of 1000\n",
      "building tree 348 of 1000\n",
      "building tree 349 of 1000\n",
      "building tree 350 of 1000\n",
      "building tree 351 of 1000\n",
      "building tree 352 of 1000\n",
      "building tree 353 of 1000\n",
      "building tree 354 of 1000\n",
      "building tree 355 of 1000\n",
      "building tree 356 of 1000\n",
      "building tree 357 of 1000\n",
      "building tree 358 of 1000\n",
      "building tree 359 of 1000\n",
      "building tree 360 of 1000\n",
      "building tree 361 of 1000\n",
      "building tree 362 of 1000\n",
      "building tree 363 of 1000\n",
      "building tree 364 of 1000\n",
      "building tree 365 of 1000\n",
      "building tree 366 of 1000\n",
      "building tree 367 of 1000\n",
      "building tree 368 of 1000\n",
      "building tree 369 of 1000\n",
      "building tree 370 of 1000\n",
      "building tree 371 of 1000\n",
      "building tree 372 of 1000\n",
      "building tree 373 of 1000\n",
      "building tree 374 of 1000\n",
      "building tree 375 of 1000\n",
      "building tree 376 of 1000\n",
      "building tree 377 of 1000\n",
      "building tree 378 of 1000\n",
      "building tree 379 of 1000\n",
      "building tree 380 of 1000\n",
      "building tree 381 of 1000\n",
      "building tree 382 of 1000\n",
      "building tree 383 of 1000\n",
      "building tree 384 of 1000\n",
      "building tree 385 of 1000\n",
      "building tree 386 of 1000\n",
      "building tree 387 of 1000\n",
      "building tree 388 of 1000\n",
      "building tree 389 of 1000\n",
      "building tree 390 of 1000\n",
      "building tree 391 of 1000\n",
      "building tree 392 of 1000\n",
      "building tree 393 of 1000\n",
      "building tree 394 of 1000\n",
      "building tree 395 of 1000\n",
      "building tree 396 of 1000\n",
      "building tree 397 of 1000\n",
      "building tree 398 of 1000\n",
      "building tree 399 of 1000\n",
      "building tree 400 of 1000\n",
      "building tree 401 of 1000\n",
      "building tree 402 of 1000\n",
      "building tree 403 of 1000\n",
      "building tree 404 of 1000\n",
      "building tree 405 of 1000\n",
      "building tree 406 of 1000\n",
      "building tree 407 of 1000\n",
      "building tree 408 of 1000\n",
      "building tree 409 of 1000\n",
      "building tree 410 of 1000\n",
      "building tree 411 of 1000\n",
      "building tree 412 of 1000\n",
      "building tree 413 of 1000\n",
      "building tree 414 of 1000\n",
      "building tree 415 of 1000\n",
      "building tree 416 of 1000\n",
      "building tree 417 of 1000\n",
      "building tree 418 of 1000\n",
      "building tree 419 of 1000\n",
      "building tree 420 of 1000\n",
      "building tree 421 of 1000\n",
      "building tree 422 of 1000\n",
      "building tree 423 of 1000\n",
      "building tree 424 of 1000\n",
      "building tree 425 of 1000\n",
      "building tree 426 of 1000\n",
      "building tree 427 of 1000\n",
      "building tree 428 of 1000\n",
      "building tree 429 of 1000\n",
      "building tree 430 of 1000\n",
      "building tree 431 of 1000\n",
      "building tree 432 of 1000\n",
      "building tree 433 of 1000\n",
      "building tree 434 of 1000\n",
      "building tree 435 of 1000\n",
      "building tree 436 of 1000\n",
      "building tree 437 of 1000\n",
      "building tree 438 of 1000\n",
      "building tree 439 of 1000\n",
      "building tree 440 of 1000\n",
      "building tree 441 of 1000\n",
      "building tree 442 of 1000\n",
      "building tree 443 of 1000\n",
      "building tree 444 of 1000\n",
      "building tree 445 of 1000\n",
      "building tree 446 of 1000\n",
      "building tree 447 of 1000\n",
      "building tree 448 of 1000\n",
      "building tree 449 of 1000\n",
      "building tree 450 of 1000\n",
      "building tree 451 of 1000\n",
      "building tree 452 of 1000\n",
      "building tree 453 of 1000\n",
      "building tree 454 of 1000\n",
      "building tree 455 of 1000\n",
      "building tree 456 of 1000\n",
      "building tree 457 of 1000\n",
      "building tree 458 of 1000\n",
      "building tree 459 of 1000\n",
      "building tree 460 of 1000\n",
      "building tree 461 of 1000\n",
      "building tree 462 of 1000\n",
      "building tree 463 of 1000\n",
      "building tree 464 of 1000\n",
      "building tree 465 of 1000\n",
      "building tree 466 of 1000\n",
      "building tree 467 of 1000\n",
      "building tree 468 of 1000\n",
      "building tree 469 of 1000\n",
      "building tree 470 of 1000\n",
      "building tree 471 of 1000\n",
      "building tree 472 of 1000\n",
      "building tree 473 of 1000\n",
      "building tree 474 of 1000\n",
      "building tree 475 of 1000\n",
      "building tree 476 of 1000\n",
      "building tree 477 of 1000\n",
      "building tree 478 of 1000\n",
      "building tree 479 of 1000\n",
      "building tree 480 of 1000\n",
      "building tree 481 of 1000\n",
      "building tree 482 of 1000\n",
      "building tree 483 of 1000\n",
      "building tree 484 of 1000\n",
      "building tree 485 of 1000\n",
      "building tree 486 of 1000\n",
      "building tree 487 of 1000\n",
      "building tree 488 of 1000\n",
      "building tree 489 of 1000\n",
      "building tree 490 of 1000\n",
      "building tree 491 of 1000\n",
      "building tree 492 of 1000\n",
      "building tree 493 of 1000\n",
      "building tree 494 of 1000\n",
      "building tree 495 of 1000\n",
      "building tree 496 of 1000\n",
      "building tree 497 of 1000\n",
      "building tree 498 of 1000\n",
      "building tree 499 of 1000\n",
      "building tree 500 of 1000\n",
      "building tree 501 of 1000\n",
      "building tree 502 of 1000\n",
      "building tree 503 of 1000\n",
      "building tree 504 of 1000\n",
      "building tree 505 of 1000\n",
      "building tree 506 of 1000\n",
      "building tree 507 of 1000\n",
      "building tree 508 of 1000\n",
      "building tree 509 of 1000\n",
      "building tree 510 of 1000\n",
      "building tree 511 of 1000\n",
      "building tree 512 of 1000\n",
      "building tree 513 of 1000\n",
      "building tree 514 of 1000\n",
      "building tree 515 of 1000\n",
      "building tree 516 of 1000\n",
      "building tree 517 of 1000\n",
      "building tree 518 of 1000\n",
      "building tree 519 of 1000\n",
      "building tree 520 of 1000\n",
      "building tree 521 of 1000\n",
      "building tree 522 of 1000\n",
      "building tree 523 of 1000\n",
      "building tree 524 of 1000\n",
      "building tree 525 of 1000\n",
      "building tree 526 of 1000\n",
      "building tree 527 of 1000\n",
      "building tree 528 of 1000\n",
      "building tree 529 of 1000\n",
      "building tree 530 of 1000\n",
      "building tree 531 of 1000\n",
      "building tree 532 of 1000\n",
      "building tree 533 of 1000\n",
      "building tree 534 of 1000\n",
      "building tree 535 of 1000\n",
      "building tree 536 of 1000\n",
      "building tree 537 of 1000\n",
      "building tree 538 of 1000\n",
      "building tree 539 of 1000\n",
      "building tree 540 of 1000\n",
      "building tree 541 of 1000\n",
      "building tree 542 of 1000\n",
      "building tree 543 of 1000\n",
      "building tree 544 of 1000\n",
      "building tree 545 of 1000\n",
      "building tree 546 of 1000\n",
      "building tree 547 of 1000\n",
      "building tree 548 of 1000\n",
      "building tree 549 of 1000\n",
      "building tree 550 of 1000\n",
      "building tree 551 of 1000\n",
      "building tree 552 of 1000\n",
      "building tree 553 of 1000\n",
      "building tree 554 of 1000\n",
      "building tree 555 of 1000\n",
      "building tree 556 of 1000\n",
      "building tree 557 of 1000\n",
      "building tree 558 of 1000\n",
      "building tree 559 of 1000\n",
      "building tree 560 of 1000\n",
      "building tree 561 of 1000\n",
      "building tree 562 of 1000\n",
      "building tree 563 of 1000\n",
      "building tree 564 of 1000\n",
      "building tree 565 of 1000\n",
      "building tree 566 of 1000\n",
      "building tree 567 of 1000\n",
      "building tree 568 of 1000\n",
      "building tree 569 of 1000\n",
      "building tree 570 of 1000\n",
      "building tree 571 of 1000\n",
      "building tree 572 of 1000\n",
      "building tree 573 of 1000\n",
      "building tree 574 of 1000\n",
      "building tree 575 of 1000\n",
      "building tree 576 of 1000\n",
      "building tree 577 of 1000\n",
      "building tree 578 of 1000\n",
      "building tree 579 of 1000\n",
      "building tree 580 of 1000\n",
      "building tree 581 of 1000\n",
      "building tree 582 of 1000\n",
      "building tree 583 of 1000\n",
      "building tree 584 of 1000\n",
      "building tree 585 of 1000\n",
      "building tree 586 of 1000\n",
      "building tree 587 of 1000\n",
      "building tree 588 of 1000\n",
      "building tree 589 of 1000\n",
      "building tree 590 of 1000\n",
      "building tree 591 of 1000\n",
      "building tree 592 of 1000\n",
      "building tree 593 of 1000\n",
      "building tree 594 of 1000\n",
      "building tree 595 of 1000\n",
      "building tree 596 of 1000\n",
      "building tree 597 of 1000\n",
      "building tree 598 of 1000\n",
      "building tree 599 of 1000\n",
      "building tree 600 of 1000\n",
      "building tree 601 of 1000\n",
      "building tree 602 of 1000\n",
      "building tree 603 of 1000\n",
      "building tree 604 of 1000\n",
      "building tree 605 of 1000\n",
      "building tree 606 of 1000\n",
      "building tree 607 of 1000\n",
      "building tree 608 of 1000\n",
      "building tree 609 of 1000\n",
      "building tree 610 of 1000\n",
      "building tree 611 of 1000\n",
      "building tree 612 of 1000\n",
      "building tree 613 of 1000\n",
      "building tree 614 of 1000\n",
      "building tree 615 of 1000\n",
      "building tree 616 of 1000\n",
      "building tree 617 of 1000\n",
      "building tree 618 of 1000\n",
      "building tree 619 of 1000\n",
      "building tree 620 of 1000\n",
      "building tree 621 of 1000\n",
      "building tree 622 of 1000\n",
      "building tree 623 of 1000\n",
      "building tree 624 of 1000\n",
      "building tree 625 of 1000\n",
      "building tree 626 of 1000\n",
      "building tree 627 of 1000\n",
      "building tree 628 of 1000\n",
      "building tree 629 of 1000\n",
      "building tree 630 of 1000\n",
      "building tree 631 of 1000\n",
      "building tree 632 of 1000\n",
      "building tree 633 of 1000\n",
      "building tree 634 of 1000\n",
      "building tree 635 of 1000\n",
      "building tree 636 of 1000\n",
      "building tree 637 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 638 of 1000\n",
      "building tree 639 of 1000\n",
      "building tree 640 of 1000\n",
      "building tree 641 of 1000\n",
      "building tree 642 of 1000\n",
      "building tree 643 of 1000\n",
      "building tree 644 of 1000\n",
      "building tree 645 of 1000\n",
      "building tree 646 of 1000\n",
      "building tree 647 of 1000\n",
      "building tree 648 of 1000\n",
      "building tree 649 of 1000\n",
      "building tree 650 of 1000\n",
      "building tree 651 of 1000\n",
      "building tree 652 of 1000\n",
      "building tree 653 of 1000\n",
      "building tree 654 of 1000\n",
      "building tree 655 of 1000\n",
      "building tree 656 of 1000\n",
      "building tree 657 of 1000\n",
      "building tree 658 of 1000\n",
      "building tree 659 of 1000\n",
      "building tree 660 of 1000\n",
      "building tree 661 of 1000\n",
      "building tree 662 of 1000\n",
      "building tree 663 of 1000\n",
      "building tree 664 of 1000\n",
      "building tree 665 of 1000\n",
      "building tree 666 of 1000\n",
      "building tree 667 of 1000\n",
      "building tree 668 of 1000\n",
      "building tree 669 of 1000\n",
      "building tree 670 of 1000\n",
      "building tree 671 of 1000\n",
      "building tree 672 of 1000\n",
      "building tree 673 of 1000\n",
      "building tree 674 of 1000\n",
      "building tree 675 of 1000\n",
      "building tree 676 of 1000\n",
      "building tree 677 of 1000\n",
      "building tree 678 of 1000\n",
      "building tree 679 of 1000\n",
      "building tree 680 of 1000\n",
      "building tree 681 of 1000\n",
      "building tree 682 of 1000\n",
      "building tree 683 of 1000\n",
      "building tree 684 of 1000\n",
      "building tree 685 of 1000\n",
      "building tree 686 of 1000\n",
      "building tree 687 of 1000\n",
      "building tree 688 of 1000\n",
      "building tree 689 of 1000\n",
      "building tree 690 of 1000\n",
      "building tree 691 of 1000\n",
      "building tree 692 of 1000\n",
      "building tree 693 of 1000\n",
      "building tree 694 of 1000\n",
      "building tree 695 of 1000\n",
      "building tree 696 of 1000\n",
      "building tree 697 of 1000\n",
      "building tree 698 of 1000\n",
      "building tree 699 of 1000\n",
      "building tree 700 of 1000\n",
      "building tree 701 of 1000\n",
      "building tree 702 of 1000\n",
      "building tree 703 of 1000\n",
      "building tree 704 of 1000\n",
      "building tree 705 of 1000\n",
      "building tree 706 of 1000\n",
      "building tree 707 of 1000\n",
      "building tree 708 of 1000\n",
      "building tree 709 of 1000\n",
      "building tree 710 of 1000\n",
      "building tree 711 of 1000\n",
      "building tree 712 of 1000\n",
      "building tree 713 of 1000\n",
      "building tree 714 of 1000\n",
      "building tree 715 of 1000\n",
      "building tree 716 of 1000\n",
      "building tree 717 of 1000\n",
      "building tree 718 of 1000\n",
      "building tree 719 of 1000\n",
      "building tree 720 of 1000\n",
      "building tree 721 of 1000\n",
      "building tree 722 of 1000\n",
      "building tree 723 of 1000\n",
      "building tree 724 of 1000\n",
      "building tree 725 of 1000\n",
      "building tree 726 of 1000\n",
      "building tree 727 of 1000\n",
      "building tree 728 of 1000\n",
      "building tree 729 of 1000\n",
      "building tree 730 of 1000\n",
      "building tree 731 of 1000\n",
      "building tree 732 of 1000\n",
      "building tree 733 of 1000\n",
      "building tree 734 of 1000\n",
      "building tree 735 of 1000\n",
      "building tree 736 of 1000\n",
      "building tree 737 of 1000\n",
      "building tree 738 of 1000\n",
      "building tree 739 of 1000\n",
      "building tree 740 of 1000\n",
      "building tree 741 of 1000\n",
      "building tree 742 of 1000\n",
      "building tree 743 of 1000\n",
      "building tree 744 of 1000\n",
      "building tree 745 of 1000\n",
      "building tree 746 of 1000\n",
      "building tree 747 of 1000\n",
      "building tree 748 of 1000\n",
      "building tree 749 of 1000\n",
      "building tree 750 of 1000\n",
      "building tree 751 of 1000\n",
      "building tree 752 of 1000\n",
      "building tree 753 of 1000\n",
      "building tree 754 of 1000\n",
      "building tree 755 of 1000\n",
      "building tree 756 of 1000\n",
      "building tree 757 of 1000\n",
      "building tree 758 of 1000\n",
      "building tree 759 of 1000\n",
      "building tree 760 of 1000\n",
      "building tree 761 of 1000\n",
      "building tree 762 of 1000\n",
      "building tree 763 of 1000\n",
      "building tree 764 of 1000\n",
      "building tree 765 of 1000\n",
      "building tree 766 of 1000\n",
      "building tree 767 of 1000\n",
      "building tree 768 of 1000\n",
      "building tree 769 of 1000\n",
      "building tree 770 of 1000\n",
      "building tree 771 of 1000\n",
      "building tree 772 of 1000\n",
      "building tree 773 of 1000\n",
      "building tree 774 of 1000\n",
      "building tree 775 of 1000\n",
      "building tree 776 of 1000\n",
      "building tree 777 of 1000\n",
      "building tree 778 of 1000\n",
      "building tree 779 of 1000\n",
      "building tree 780 of 1000\n",
      "building tree 781 of 1000\n",
      "building tree 782 of 1000\n",
      "building tree 783 of 1000\n",
      "building tree 784 of 1000\n",
      "building tree 785 of 1000\n",
      "building tree 786 of 1000\n",
      "building tree 787 of 1000\n",
      "building tree 788 of 1000\n",
      "building tree 789 of 1000\n",
      "building tree 790 of 1000\n",
      "building tree 791 of 1000\n",
      "building tree 792 of 1000\n",
      "building tree 793 of 1000\n",
      "building tree 794 of 1000\n",
      "building tree 795 of 1000\n",
      "building tree 796 of 1000\n",
      "building tree 797 of 1000\n",
      "building tree 798 of 1000\n",
      "building tree 799 of 1000\n",
      "building tree 800 of 1000\n",
      "building tree 801 of 1000\n",
      "building tree 802 of 1000\n",
      "building tree 803 of 1000\n",
      "building tree 804 of 1000\n",
      "building tree 805 of 1000\n",
      "building tree 806 of 1000\n",
      "building tree 807 of 1000\n",
      "building tree 808 of 1000\n",
      "building tree 809 of 1000\n",
      "building tree 810 of 1000\n",
      "building tree 811 of 1000\n",
      "building tree 812 of 1000\n",
      "building tree 813 of 1000\n",
      "building tree 814 of 1000\n",
      "building tree 815 of 1000\n",
      "building tree 816 of 1000\n",
      "building tree 817 of 1000\n",
      "building tree 818 of 1000\n",
      "building tree 819 of 1000\n",
      "building tree 820 of 1000\n",
      "building tree 821 of 1000\n",
      "building tree 822 of 1000\n",
      "building tree 823 of 1000\n",
      "building tree 824 of 1000\n",
      "building tree 825 of 1000\n",
      "building tree 826 of 1000\n",
      "building tree 827 of 1000\n",
      "building tree 828 of 1000\n",
      "building tree 829 of 1000\n",
      "building tree 830 of 1000\n",
      "building tree 831 of 1000\n",
      "building tree 832 of 1000\n",
      "building tree 833 of 1000\n",
      "building tree 834 of 1000\n",
      "building tree 835 of 1000\n",
      "building tree 836 of 1000\n",
      "building tree 837 of 1000\n",
      "building tree 838 of 1000\n",
      "building tree 839 of 1000\n",
      "building tree 840 of 1000\n",
      "building tree 841 of 1000\n",
      "building tree 842 of 1000\n",
      "building tree 843 of 1000\n",
      "building tree 844 of 1000\n",
      "building tree 845 of 1000\n",
      "building tree 846 of 1000\n",
      "building tree 847 of 1000\n",
      "building tree 848 of 1000\n",
      "building tree 849 of 1000\n",
      "building tree 850 of 1000\n",
      "building tree 851 of 1000\n",
      "building tree 852 of 1000\n",
      "building tree 853 of 1000\n",
      "building tree 854 of 1000\n",
      "building tree 855 of 1000\n",
      "building tree 856 of 1000\n",
      "building tree 857 of 1000\n",
      "building tree 858 of 1000\n",
      "building tree 859 of 1000\n",
      "building tree 860 of 1000\n",
      "building tree 861 of 1000\n",
      "building tree 862 of 1000\n",
      "building tree 863 of 1000\n",
      "building tree 864 of 1000\n",
      "building tree 865 of 1000\n",
      "building tree 866 of 1000\n",
      "building tree 867 of 1000\n",
      "building tree 868 of 1000\n",
      "building tree 869 of 1000\n",
      "building tree 870 of 1000\n",
      "building tree 871 of 1000\n",
      "building tree 872 of 1000\n",
      "building tree 873 of 1000\n",
      "building tree 874 of 1000\n",
      "building tree 875 of 1000\n",
      "building tree 876 of 1000\n",
      "building tree 877 of 1000\n",
      "building tree 878 of 1000\n",
      "building tree 879 of 1000\n",
      "building tree 880 of 1000\n",
      "building tree 881 of 1000\n",
      "building tree 882 of 1000\n",
      "building tree 883 of 1000\n",
      "building tree 884 of 1000\n",
      "building tree 885 of 1000\n",
      "building tree 886 of 1000\n",
      "building tree 887 of 1000\n",
      "building tree 888 of 1000\n",
      "building tree 889 of 1000\n",
      "building tree 890 of 1000\n",
      "building tree 891 of 1000\n",
      "building tree 892 of 1000\n",
      "building tree 893 of 1000\n",
      "building tree 894 of 1000\n",
      "building tree 895 of 1000\n",
      "building tree 896 of 1000\n",
      "building tree 897 of 1000\n",
      "building tree 898 of 1000\n",
      "building tree 899 of 1000\n",
      "building tree 900 of 1000\n",
      "building tree 901 of 1000\n",
      "building tree 902 of 1000\n",
      "building tree 903 of 1000\n",
      "building tree 904 of 1000\n",
      "building tree 905 of 1000\n",
      "building tree 906 of 1000\n",
      "building tree 907 of 1000\n",
      "building tree 908 of 1000\n",
      "building tree 909 of 1000\n",
      "building tree 910 of 1000\n",
      "building tree 911 of 1000\n",
      "building tree 912 of 1000\n",
      "building tree 913 of 1000\n",
      "building tree 914 of 1000\n",
      "building tree 915 of 1000\n",
      "building tree 916 of 1000\n",
      "building tree 917 of 1000\n",
      "building tree 918 of 1000\n",
      "building tree 919 of 1000\n",
      "building tree 920 of 1000\n",
      "building tree 921 of 1000\n",
      "building tree 922 of 1000\n",
      "building tree 923 of 1000\n",
      "building tree 924 of 1000\n",
      "building tree 925 of 1000\n",
      "building tree 926 of 1000\n",
      "building tree 927 of 1000\n",
      "building tree 928 of 1000\n",
      "building tree 929 of 1000\n",
      "building tree 930 of 1000\n",
      "building tree 931 of 1000\n",
      "building tree 932 of 1000\n",
      "building tree 933 of 1000\n",
      "building tree 934 of 1000\n",
      "building tree 935 of 1000\n",
      "building tree 936 of 1000\n",
      "building tree 937 of 1000\n",
      "building tree 938 of 1000\n",
      "building tree 939 of 1000\n",
      "building tree 940 of 1000\n",
      "building tree 941 of 1000\n",
      "building tree 942 of 1000\n",
      "building tree 943 of 1000\n",
      "building tree 944 of 1000\n",
      "building tree 945 of 1000\n",
      "building tree 946 of 1000\n",
      "building tree 947 of 1000\n",
      "building tree 948 of 1000\n",
      "building tree 949 of 1000\n",
      "building tree 950 of 1000\n",
      "building tree 951 of 1000\n",
      "building tree 952 of 1000\n",
      "building tree 953 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 954 of 1000\n",
      "building tree 955 of 1000\n",
      "building tree 956 of 1000\n",
      "building tree 957 of 1000\n",
      "building tree 958 of 1000\n",
      "building tree 959 of 1000\n",
      "building tree 960 of 1000\n",
      "building tree 961 of 1000\n",
      "building tree 962 of 1000\n",
      "building tree 963 of 1000\n",
      "building tree 964 of 1000\n",
      "building tree 965 of 1000\n",
      "building tree 966 of 1000\n",
      "building tree 967 of 1000\n",
      "building tree 968 of 1000\n",
      "building tree 969 of 1000\n",
      "building tree 970 of 1000\n",
      "building tree 971 of 1000\n",
      "building tree 972 of 1000\n",
      "building tree 973 of 1000\n",
      "building tree 974 of 1000\n",
      "building tree 975 of 1000\n",
      "building tree 976 of 1000\n",
      "building tree 977 of 1000\n",
      "building tree 978 of 1000\n",
      "building tree 979 of 1000\n",
      "building tree 980 of 1000\n",
      "building tree 981 of 1000\n",
      "building tree 982 of 1000\n",
      "building tree 983 of 1000\n",
      "building tree 984 of 1000\n",
      "building tree 985 of 1000\n",
      "building tree 986 of 1000\n",
      "building tree 987 of 1000\n",
      "building tree 988 of 1000\n",
      "building tree 989 of 1000\n",
      "building tree 990 of 1000\n",
      "building tree 991 of 1000\n",
      "building tree 992 of 1000\n",
      "building tree 993 of 1000\n",
      "building tree 994 of 1000\n",
      "building tree 995 of 1000\n",
      "building tree 996 of 1000\n",
      "building tree 997 of 1000\n",
      "building tree 998 of 1000\n",
      "building tree 999 of 1000\n",
      "building tree 1000 of 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=1000, n_estimators=1000, random_state=0,\n",
       "                      verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # create regressor object\n",
    "regressor = RandomForestRegressor(n_estimators = 1000, random_state = 0,max_depth=1000, verbose=2)\n",
    "  \n",
    "# fit the regressor with x and y data\n",
    "regressor.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.3227485]\n",
      " [0.3227485 1.       ]]\n",
      "Error during validation:  0.031966919240253897\n"
     ]
    }
   ],
   "source": [
    "y_prediction = regressor.predict(val_features)\n",
    "y_prediction = np.squeeze(y_prediction)\n",
    "plt.scatter(val_targets, y_prediction)\n",
    "plt.xlabel('Targets')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "my_rho = np.corrcoef(val_targets, y_prediction)\n",
    "print(my_rho)\n",
    "err = (np.linalg.norm(y_prediction-val_targets))**2/len(y_prediction)\n",
    "print('Error during validation: ',err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x212b0576788>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(y_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, even though the results are not what we expected, there is an important improvement compared with the first approach (better corelation and distribution). Nevertheless, there is a lot of room for more improvement. Notably, we can work with the fact that both models failed to predict the deviation of the data, targeting excessively the mean values. We could fix this problem by over-sampling the songs that are closer to the extreme values 0 and 100 and see if this helps our models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
